{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41fada50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:03:02.359088Z",
     "iopub.status.busy": "2024-06-16T13:03:02.358727Z",
     "iopub.status.idle": "2024-06-16T13:03:18.974568Z",
     "shell.execute_reply": "2024-06-16T13:03:18.972998Z"
    },
    "papermill": {
     "duration": 16.628627,
     "end_time": "2024-06-16T13:03:18.977310",
     "exception": false,
     "start_time": "2024-06-16T13:03:02.348683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2+cpu)\r\n",
      "Collecting torch-geometric\r\n",
      "  Downloading torch_geometric-2.5.3-py3-none-any.whl.metadata (64 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12.1)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.3.1)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (4.66.4)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (1.26.4)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (1.11.4)\r\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (3.9.1)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (2.32.3)\r\n",
      "Requirement already satisfied: pyparsing in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (3.1.1)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (1.2.2)\r\n",
      "Requirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (5.9.3)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (23.2.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (6.0.4)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.9.3)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.4.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.3.1)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (4.0.3)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (2024.2.2)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->torch-geometric) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->torch-geometric) (3.2.0)\r\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\r\n",
      "Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: torch-geometric\r\n",
      "Successfully installed torch-geometric-2.5.3\r\n"
     ]
    }
   ],
   "source": [
    "DATASET = 'PROTEINS'\n",
    "if 'first_run' not in globals():\n",
    "    !pip install torch torch-geometric\n",
    "    first_run = False\n",
    "else:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6f986b2",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-06-16T13:03:18.998175Z",
     "iopub.status.busy": "2024-06-16T13:03:18.997768Z",
     "iopub.status.idle": "2024-06-16T13:03:26.895494Z",
     "shell.execute_reply": "2024-06-16T13:03:26.894217Z"
    },
    "papermill": {
     "duration": 7.911536,
     "end_time": "2024-06-16T13:03:26.898488",
     "exception": false,
     "start_time": "2024-06-16T13:03:18.986952",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.transforms import BaseTransform\n",
    "from torch_geometric.utils import to_scipy_sparse_matrix\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.io import fs, read_txt_array\n",
    "import torch_geometric.transforms as T\n",
    "# from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "import math\n",
    "from typing import Callable, List, Optional, Tuple, Dict\n",
    "from torch import Tensor\n",
    "import os\n",
    "import requests\n",
    "from torch_geometric.utils import coalesce, cumsum, one_hot, remove_self_loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b734c1a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:03:26.920068Z",
     "iopub.status.busy": "2024-06-16T13:03:26.919452Z",
     "iopub.status.idle": "2024-06-16T13:03:26.926252Z",
     "shell.execute_reply": "2024-06-16T13:03:26.924764Z"
    },
    "papermill": {
     "duration": 0.020254,
     "end_time": "2024-06-16T13:03:26.928755",
     "exception": false,
     "start_time": "2024-06-16T13:03:26.908501",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class MyTransform(BaseTransform):\n",
    "#     def __init__(self, set_ids = False, indices = None, set_initial_features=False, embedding=None, cos_sim=None, z_score=None):\n",
    "#         super(MyTransform, self).__init__()\n",
    "#         self.current_id = 0  # Initialize a counter for unique IDs\n",
    "#         self.set_initial_features = set_initial_features\n",
    "#         self.embedding = embedding  # Assume this is a function if not None\n",
    "#         self.cos_sim = cos_sim      # Assume this is a dictionary or list if not None\n",
    "#         self.z_score = z_score      # Assume this is a dictionary or list if not None\n",
    "#         self.indices = indices\n",
    "#         if set_ids:\n",
    "#             self.assign_ids()\n",
    "\n",
    "#     def assign_ids(self):\n",
    "#         # Create an ID tensor for all data points\n",
    "#         id_tensor = torch.arange(self.indices)\n",
    "#         for i, data in enumerate(self):\n",
    "#             data.id = id_tensor[i]\n",
    "\n",
    "#     def __call__(self, data: Data) -> Data:\n",
    "#         # Assign a unique ID and increment the counter\n",
    "#         if self.set_initial_features:\n",
    "#             data.id = self.current_id\n",
    "#             self.current_id += 1\n",
    "#             data.num_edges = data.edge_index.size(1)  # Number of edges\n",
    "#             data.adj_mat = self.to_numpy_adj(data)  # Convert edge index to adjacency matrix\n",
    "\n",
    "#         if self.embedding:\n",
    "#             data.embedding = self.embedding[data.id] # Assume embedding function takes Data and modifies it\n",
    "\n",
    "#         if self.cos_sim is not None:\n",
    "#             data.cos_sim = self.cos_sim[data.id]  # Fetch cosine similarity based on ID\n",
    "\n",
    "#         if self.z_score is not None:\n",
    "#             data.z_score = self.z_score[data.id]  # Fetch z-score based on ID\n",
    "\n",
    "#         return data\n",
    "\n",
    "#     def to_numpy_adj(self, data):\n",
    "#         # Creates an adjacency matrix from edge_index\n",
    "#         adj_mat = np.zeros((data.num_nodes, data.num_nodes))\n",
    "#         edge_index = data.edge_index.numpy()\n",
    "#         adj_mat[edge_index[0], edge_index[1]] = 1\n",
    "#         return adj_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b71b693d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:03:26.950829Z",
     "iopub.status.busy": "2024-06-16T13:03:26.950398Z",
     "iopub.status.idle": "2024-06-16T13:03:26.958270Z",
     "shell.execute_reply": "2024-06-16T13:03:26.956931Z"
    },
    "papermill": {
     "duration": 0.021806,
     "end_time": "2024-06-16T13:03:26.960846",
     "exception": false,
     "start_time": "2024-06-16T13:03:26.939040",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class IDAddingDataset:\n",
    "    def __init__(self, dataset, attr_func):\n",
    "        self.dataset = dataset\n",
    "        self.attr_func = attr_func\n",
    "        self.indices = np.arange(0, len(dataset.indices()))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.dataset[idx].clone()  # Clone the data to avoid modifying the original dataset\n",
    "        data.id = self.indices[idx]\n",
    "        data.num_edges = self.dataset[idx].edge_index.size(1)\n",
    "        data.adj_mat = to_scipy_sparse_matrix(self.dataset[idx].edge_index, num_nodes=self.dataset[idx].num_nodes).toarray().astype(int)\n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aee623fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:03:26.982755Z",
     "iopub.status.busy": "2024-06-16T13:03:26.982285Z",
     "iopub.status.idle": "2024-06-16T13:03:26.989103Z",
     "shell.execute_reply": "2024-06-16T13:03:26.987906Z"
    },
    "papermill": {
     "duration": 0.020552,
     "end_time": "2024-06-16T13:03:26.991671",
     "exception": false,
     "start_time": "2024-06-16T13:03:26.971119",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EmbeddingAddingDataset:\n",
    "    def __init__(self, dataset, embedding_dict):\n",
    "        self.dataset = dataset\n",
    "        self.embedding_dict = embedding_dict\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.dataset[idx].clone()  # Clone the data to avoid modifying the original dataset\n",
    "        data.embedding = self.embedding_dict[self.dataset[idx].id]\n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccaf77a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:03:27.012597Z",
     "iopub.status.busy": "2024-06-16T13:03:27.011648Z",
     "iopub.status.idle": "2024-06-16T13:03:27.019071Z",
     "shell.execute_reply": "2024-06-16T13:03:27.017868Z"
    },
    "papermill": {
     "duration": 0.020376,
     "end_time": "2024-06-16T13:03:27.021444",
     "exception": false,
     "start_time": "2024-06-16T13:03:27.001068",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FinalAddingDataset:\n",
    "    def __init__(self, Embedding_dataset, cos_sims, all_z_scores):\n",
    "        self.dataset = Embedding_dataset\n",
    "        self.cos_sims = cos_sims\n",
    "        self.all_z_scores = all_z_scores\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.dataset[idx].clone()  # Clone the data to avoid modifying the original dataset\n",
    "        data.cos_sim = self.cos_sims[self.dataset[idx].id]\n",
    "        data.z_score = self.all_z_scores[self.dataset[idx].id]\n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4755f25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:03:27.042495Z",
     "iopub.status.busy": "2024-06-16T13:03:27.042066Z",
     "iopub.status.idle": "2024-06-16T13:03:27.051541Z",
     "shell.execute_reply": "2024-06-16T13:03:27.050068Z"
    },
    "papermill": {
     "duration": 0.022575,
     "end_time": "2024-06-16T13:03:27.053882",
     "exception": false,
     "start_time": "2024-06-16T13:03:27.031307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def stable_hash(value):\n",
    "    return hashlib.md5(str(value).encode()).hexdigest()\n",
    "def get_WL_embedding(data, n_iter):\n",
    "    graph = data.adj_mat\n",
    "    graph_hash_dict = {}\n",
    "    labels = [np.sum(graph[x]) for x in range(len(graph))]  # Initialize labels based on node degrees\n",
    "    for _ in range(n_iter):\n",
    "        neighbor_labels = [sorted([labels[j] for j in range(len(graph)) if graph[i, j] == 1]) for i in range(len(graph))]\n",
    "        hashes = np.array([stable_hash((labels[i], tuple(neighbor_labels[i]))) for i in range(len(graph))])\n",
    "        \n",
    "        for unique_hash in set(hashes):\n",
    "            graph_hash_dict[unique_hash] = np.sum(hashes == unique_hash)\n",
    "\n",
    "        labels = hashes.tolist()\n",
    "\n",
    "    return graph_hash_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95b95e7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:03:27.074619Z",
     "iopub.status.busy": "2024-06-16T13:03:27.074183Z",
     "iopub.status.idle": "2024-06-16T13:03:27.089683Z",
     "shell.execute_reply": "2024-06-16T13:03:27.088378Z"
    },
    "papermill": {
     "duration": 0.028883,
     "end_time": "2024-06-16T13:03:27.092199",
     "exception": false,
     "start_time": "2024-06-16T13:03:27.063316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_mean_vectors(dataset):\n",
    "    all_hashes = set()\n",
    "    all_classes = set()\n",
    "    for graph in dataset:\n",
    "        all_hashes.update(graph.embedding.keys())\n",
    "        all_classes.add(graph.y.item())\n",
    "    all_classes.add('all')\n",
    "\n",
    "    vector_sum_dict = {_cls: {h: 0 for h in all_hashes} for _cls in all_classes}\n",
    "    category_sum_dict = {_cls: 0 for _cls in all_classes}\n",
    "    mean_vector_dict = {_cls: {h: 0 for h in all_hashes} for _cls in all_classes}\n",
    "    \n",
    "    \n",
    "    for graph in dataset:\n",
    "        for _hash, count in graph.embedding.items():\n",
    "            vector_sum_dict[graph.y.item()][_hash] += count\n",
    "            category_sum_dict[graph.y.item()] += 1\n",
    "            vector_sum_dict['all'][_hash] += count\n",
    "            category_sum_dict['all'] += 1\n",
    "    for _cls, hash_counts in vector_sum_dict.items():\n",
    "        for _hash, count in hash_counts.items():\n",
    "            mean_vector_dict[_cls][_hash] = count/category_sum_dict[_cls]\n",
    "    return all_hashes, all_classes, mean_vector_dict\n",
    "\n",
    "def get_cos_sim(dataset, all_hashes, all_classes, mean_vectors):\n",
    "    cos_sims, norm_dict = {}, {}\n",
    "    graph_norm = math.sqrt(sum(count**2 for count in graph.embedding.values()))\n",
    "    for _class in all_classes:\n",
    "        norm_dict[_class] = math.sqrt(sum(count**2 for count in mean_vectors[_class].values()))\n",
    "    all_numerator = sum(count * mean_vectors['all'][_hash] for _hash, count in graph.embedding.items())\n",
    "    all_denom = norm_dict['all'] * graph_norm\n",
    "    cos_sims['all'] = all_numerator / all_denom\n",
    "\n",
    "    class_numerator = sum(count * mean_vectors[graph.y.item()][_hash] for _hash, count in graph.embedding.items())\n",
    "    class_denom = norm_dict[graph.y.item()] * graph_norm\n",
    "    cos_sims['class'] = class_numerator / class_denom\n",
    "    \n",
    "    return cos_sims\n",
    "\n",
    "def calc_z_scores(graph, cos_sims, class_cos_sims):\n",
    "    # Ensure data types are numpy arrays for statistical computation\n",
    "    all_cos_sims = np.array(class_cos_sims['all'])\n",
    "    cat_cos_sims = np.array(class_cos_sims[graph.y.item()])\n",
    "    \n",
    "    # Calculate means and standard deviations for 'all' and specific 'class'\n",
    "    all_mean = np.mean(all_cos_sims)\n",
    "    cat_mean = np.mean(cat_cos_sims)\n",
    "    all_std_dev = np.std(all_cos_sims)\n",
    "    class_std_dev = np.std(cat_cos_sims)\n",
    "    \n",
    "    # Compute z-scores using the standard formula, include flooring to nearest 0.5 if needed\n",
    "    z_scores = {\n",
    "        'all': (cos_sims[graph.id]['all']) / all_std_dev,\n",
    "        'class': (cos_sims[graph.id]['class']) / class_std_dev\n",
    "    }\n",
    "    \n",
    "    return z_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "263e732a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:03:27.113598Z",
     "iopub.status.busy": "2024-06-16T13:03:27.113167Z",
     "iopub.status.idle": "2024-06-16T13:03:27.118134Z",
     "shell.execute_reply": "2024-06-16T13:03:27.116906Z"
    },
    "papermill": {
     "duration": 0.019183,
     "end_time": "2024-06-16T13:03:27.120808",
     "exception": false,
     "start_time": "2024-06-16T13:03:27.101625",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from torch_geometric.io.tu import read_tu_data\n",
    "# data, slices, sizes = read_tu_data('/kaggle/working/', 'ENZYMES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e35a55d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:03:27.142966Z",
     "iopub.status.busy": "2024-06-16T13:03:27.142545Z",
     "iopub.status.idle": "2024-06-16T13:03:27.159722Z",
     "shell.execute_reply": "2024-06-16T13:03:27.158500Z"
    },
    "papermill": {
     "duration": 0.031453,
     "end_time": "2024-06-16T13:03:27.162498",
     "exception": false,
     "start_time": "2024-06-16T13:03:27.131045",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os.path as osp\n",
    "# from typing import Callable, List, Optional\n",
    "\n",
    "# from torch_geometric.data import Data, InMemoryDataset\n",
    "# from torch_geometric.io import fs#, read_tu_data\n",
    "\n",
    "# def read_file(\n",
    "#     folder: str,\n",
    "#     prefix: str,\n",
    "#     name: str,\n",
    "#     dtype: Optional[torch.dtype] = None,\n",
    "# ) -> Tensor:\n",
    "#     path = osp.join(folder, f'{prefix}_{name}.txt')\n",
    "#     return read_txt_array(path, sep=',', dtype=dtype)\n",
    "# def cat(seq: List[Optional[Tensor]]) -> Optional[Tensor]:\n",
    "#     values = [v for v in seq if v is not None]\n",
    "#     values = [v for v in values if v.numel() > 0]\n",
    "#     values = [v.unsqueeze(-1) if v.dim() == 1 else v for v in values]\n",
    "#     return torch.cat(values, dim=-1) if len(values) > 0 else None\n",
    "# def split(data: Data, batch: Tensor) -> Tuple[Data, Dict[str, Tensor]]:\n",
    "#     node_slice = cumsum(torch.from_numpy(np.bincount(batch)))\n",
    "\n",
    "#     assert data.edge_index is not None\n",
    "#     row, _ = data.edge_index\n",
    "#     edge_slice = cumsum(torch.from_numpy(np.bincount(batch[row])))\n",
    "\n",
    "#     # Edge indices should start at zero for every graph.\n",
    "#     data.edge_index -= node_slice[batch[row]].unsqueeze(0)\n",
    "\n",
    "#     slices = {'edge_index': edge_slice}\n",
    "#     if data.x is not None:\n",
    "#         slices['x'] = node_slice\n",
    "#     else:\n",
    "#         # Imitate `collate` functionality:\n",
    "#         data._num_nodes = torch.bincount(batch).tolist()\n",
    "#         data.num_nodes = batch.numel()\n",
    "#     if data.edge_attr is not None:\n",
    "#         slices['edge_attr'] = edge_slice\n",
    "#     if data.y is not None:\n",
    "#         assert isinstance(data.y, Tensor)\n",
    "#         if data.y.size(0) == batch.size(0):\n",
    "#             slices['y'] = node_slice\n",
    "#         else:\n",
    "#             slices['y'] = torch.arange(0, int(batch[-1]) + 2, dtype=torch.long)\n",
    "\n",
    "#     return data, slices\n",
    "# def read_tu_data(\n",
    "#     folder: str,\n",
    "#     prefix: str,\n",
    "# ) -> Tuple[Data, Dict[str, Tensor], Dict[str, int]]:\n",
    "#     files = fs.glob(osp.join(folder, f'{prefix}_*.txt'))\n",
    "#     names = [osp.basename(f)[len(prefix) + 1:-4] for f in files]\n",
    "\n",
    "#     edge_index = read_file(folder, prefix, 'A', torch.long).t() - 1\n",
    "#     batch = read_file(folder, prefix, 'graph_indicator', torch.long) - 1\n",
    "\n",
    "#     node_attribute = torch.empty((batch.size(0), 0))\n",
    "#     if 'node_attributes' in names:\n",
    "#         node_attribute = read_file(folder, prefix, 'node_attributes')\n",
    "#         if node_attribute.dim() == 1:\n",
    "#             node_attribute = node_attribute.unsqueeze(-1)\n",
    "\n",
    "#     node_label = torch.empty((batch.size(0), 0))\n",
    "#     if 'node_labels' in names:\n",
    "#         node_label = read_file(folder, prefix, 'node_labels', torch.long)\n",
    "#         if node_label.dim() == 1:\n",
    "#             node_label = node_label.unsqueeze(-1)\n",
    "#         node_label = node_label - node_label.min(dim=0)[0]\n",
    "#         node_labels = list(node_label.unbind(dim=-1))\n",
    "#         node_labels = [one_hot(x) for x in node_labels]\n",
    "#         if len(node_labels) == 1:\n",
    "#             node_label = node_labels[0]\n",
    "#         else:\n",
    "#             node_label = torch.cat(node_labels, dim=-1)\n",
    "\n",
    "#     edge_attribute = torch.empty((edge_index.size(1), 0))\n",
    "#     if 'edge_attributes' in names:\n",
    "#         edge_attribute = read_file(folder, prefix, 'edge_attributes')\n",
    "#         if edge_attribute.dim() == 1:\n",
    "#             edge_attribute = edge_attribute.unsqueeze(-1)\n",
    "\n",
    "#     edge_label = torch.empty((edge_index.size(1), 0))\n",
    "#     if 'edge_labels' in names:\n",
    "#         edge_label = read_file(folder, prefix, 'edge_labels', torch.long)\n",
    "#         if edge_label.dim() == 1:\n",
    "#             edge_label = edge_label.unsqueeze(-1)\n",
    "#         edge_label = edge_label - edge_label.min(dim=0)[0]\n",
    "#         edge_labels = list(edge_label.unbind(dim=-1))\n",
    "#         edge_labels = [one_hot(e) for e in edge_labels]\n",
    "#         if len(edge_labels) == 1:\n",
    "#             edge_label = edge_labels[0]\n",
    "#         else:\n",
    "#             edge_label = torch.cat(edge_labels, dim=-1)\n",
    "\n",
    "#     x = cat([node_attribute, node_label])\n",
    "#     edge_attr = cat([edge_attribute, edge_label])\n",
    "\n",
    "#     y = None\n",
    "#     if 'graph_attributes' in names:  # Regression problem.\n",
    "#         y = read_file(folder, prefix, 'graph_attributes')\n",
    "#     elif 'graph_labels' in names:  # Classification problem.\n",
    "#         y = read_file(folder, prefix, 'graph_labels', torch.long)\n",
    "#         _, y = y.unique(sorted=True, return_inverse=True)\n",
    "\n",
    "#     num_nodes = int(edge_index.max()) + 1 if x is None else x.size(0)\n",
    "#     edge_index, edge_attr = remove_self_loops(edge_index, edge_attr)\n",
    "#     edge_index, edge_attr = coalesce(edge_index, edge_attr, num_nodes)\n",
    "#     cos_sim = torch.tensor([-1.0])\n",
    "#     z_score = torch.tensor([-1.0])\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#     data = Data(\n",
    "#         x=x,\n",
    "#         edge_index=edge_index,\n",
    "#         edge_attr=edge_attr,\n",
    "#         y=y,\n",
    "#         cos_sim=torch.tensor([-1.0]),  # Custom attribute\n",
    "#         z_score=torch.tensor([-1.0])   # Custom attribute\n",
    "#     )\n",
    "#     data, slices = split(data, batch)\n",
    "\n",
    "#     sizes = {\n",
    "#         'num_node_attributes': node_attribute.size(-1),\n",
    "#         'num_node_labels': node_label.size(-1),\n",
    "#         'num_edge_attributes': edge_attribute.size(-1),\n",
    "#         'num_edge_labels': edge_label.size(-1),\n",
    "#     }\n",
    "\n",
    "#     return data, slices, sizes\n",
    "\n",
    "# class MyTUDataset(InMemoryDataset):\n",
    "#     r\"\"\"A variety of graph kernel benchmark datasets, *.e.g.*,\n",
    "#     :obj:`\"IMDB-BINARY\"`, :obj:`\"REDDIT-BINARY\"` or :obj:`\"PROTEINS\"`,\n",
    "#     collected from the `TU Dortmund University\n",
    "#     <https://chrsmrrs.github.io/datasets>`_.\n",
    "#     In addition, this dataset wrapper provides `cleaned dataset versions\n",
    "#     <https://github.com/nd7141/graph_datasets>`_ as motivated by the\n",
    "#     `\"Understanding Isomorphism Bias in Graph Data Sets\"\n",
    "#     <https://arxiv.org/abs/1910.12091>`_ paper, containing only non-isomorphic\n",
    "#     graphs.\n",
    "\n",
    "#     .. note::\n",
    "#         Some datasets may not come with any node labels.\n",
    "#         You can then either make use of the argument :obj:`use_node_attr`\n",
    "#         to load additional continuous node attributes (if present) or provide\n",
    "#         synthetic node features using transforms such as\n",
    "#         :class:`torch_geometric.transforms.Constant` or\n",
    "#         :class:`torch_geometric.transforms.OneHotDegree`.\n",
    "\n",
    "#     Args:\n",
    "#         root (str): Root directory where the dataset should be saved.\n",
    "#         name (str): The `name\n",
    "#             <https://chrsmrrs.github.io/datasets/docs/datasets/>`_ of the\n",
    "#             dataset.\n",
    "#         transform (callable, optional): A function/transform that takes in an\n",
    "#             :obj:`torch_geometric.data.Data` object and returns a transformed\n",
    "#             version. The data object will be transformed before every access.\n",
    "#             (default: :obj:`None`)\n",
    "#         pre_transform (callable, optional): A function/transform that takes in\n",
    "#             an :obj:`torch_geometric.data.Data` object and returns a\n",
    "#             transformed version. The data object will be transformed before\n",
    "#             being saved to disk. (default: :obj:`None`)\n",
    "#         pre_filter (callable, optional): A function that takes in an\n",
    "#             :obj:`torch_geometric.data.Data` object and returns a boolean\n",
    "#             value, indicating whether the data object should be included in the\n",
    "#             final dataset. (default: :obj:`None`)\n",
    "#         force_reload (bool, optional): Whether to re-process the dataset.\n",
    "#             (default: :obj:`False`)\n",
    "#         use_node_attr (bool, optional): If :obj:`True`, the dataset will\n",
    "#             contain additional continuous node attributes (if present).\n",
    "#             (default: :obj:`False`)\n",
    "#         use_edge_attr (bool, optional): If :obj:`True`, the dataset will\n",
    "#             contain additional continuous edge attributes (if present).\n",
    "#             (default: :obj:`False`)\n",
    "#         cleaned (bool, optional): If :obj:`True`, the dataset will\n",
    "#             contain only non-isomorphic graphs. (default: :obj:`False`)\n",
    "\n",
    "#     **STATS:**\n",
    "\n",
    "#     .. list-table::\n",
    "#         :widths: 20 10 10 10 10 10\n",
    "#         :header-rows: 1\n",
    "\n",
    "#         * - Name\n",
    "#           - #graphs\n",
    "#           - #nodes\n",
    "#           - #edges\n",
    "#           - #features\n",
    "#           - #classes\n",
    "#         * - MUTAG\n",
    "#           - 188\n",
    "#           - ~17.9\n",
    "#           - ~39.6\n",
    "#           - 7\n",
    "#           - 2\n",
    "#         * - ENZYMES\n",
    "#           - 600\n",
    "#           - ~32.6\n",
    "#           - ~124.3\n",
    "#           - 3\n",
    "#           - 6\n",
    "#         * - PROTEINS\n",
    "#           - 1,113\n",
    "#           - ~39.1\n",
    "#           - ~145.6\n",
    "#           - 3\n",
    "#           - 2\n",
    "#         * - COLLAB\n",
    "#           - 5,000\n",
    "#           - ~74.5\n",
    "#           - ~4914.4\n",
    "#           - 0\n",
    "#           - 3\n",
    "#         * - IMDB-BINARY\n",
    "#           - 1,000\n",
    "#           - ~19.8\n",
    "#           - ~193.1\n",
    "#           - 0\n",
    "#           - 2\n",
    "#         * - REDDIT-BINARY\n",
    "#           - 2,000\n",
    "#           - ~429.6\n",
    "#           - ~995.5\n",
    "#           - 0\n",
    "#           - 2\n",
    "#         * - ...\n",
    "#           -\n",
    "#           -\n",
    "#           -\n",
    "#           -\n",
    "#           -\n",
    "#     \"\"\"\n",
    "\n",
    "#     url = 'https://www.chrsmrrs.com/graphkerneldatasets'\n",
    "#     cleaned_url = ('https://raw.githubusercontent.com/nd7141/'\n",
    "#                    'graph_datasets/master/datasets')\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         root: str,\n",
    "#         name: str,\n",
    "#         transform: Optional[Callable] = None,\n",
    "#         pre_transform: Optional[Callable] = None,\n",
    "#         pre_filter: Optional[Callable] = None,\n",
    "#         force_reload: bool = False,\n",
    "#         use_node_attr: bool = False,\n",
    "#         use_edge_attr: bool = False,\n",
    "#         cleaned: bool = False,\n",
    "#     ) -> None:\n",
    "#         self.name = name\n",
    "#         self.cleaned = cleaned\n",
    "#         super().__init__(root, transform, pre_transform, pre_filter,\n",
    "#                          force_reload=force_reload)\n",
    "\n",
    "#         out = fs.torch_load(self.processed_paths[0])\n",
    "#         if not isinstance(out, tuple) or len(out) < 3:\n",
    "#             raise RuntimeError(\n",
    "#                 \"The 'data' object was created by an older version of PyG. \"\n",
    "#                 \"If this error occurred while loading an already existing \"\n",
    "#                 \"dataset, remove the 'processed/' directory in the dataset's \"\n",
    "#                 \"root folder and try again.\")\n",
    "#         assert len(out) == 3 or len(out) == 4\n",
    "\n",
    "#         if len(out) == 3:  # Backward compatibility.\n",
    "#             data, self.slices, self.sizes = out\n",
    "#             data_cls = Data\n",
    "#         else:\n",
    "#             data, self.slices, self.sizes, data_cls = out\n",
    "\n",
    "#         if not isinstance(data, dict):  # Backward compatibility.\n",
    "#             self.data = data\n",
    "#         else:\n",
    "#             self.data = data_cls.from_dict(data)\n",
    "\n",
    "#         assert isinstance(self._data, Data)\n",
    "#         if self._data.x is not None and not use_node_attr:\n",
    "#             num_node_attributes = self.num_node_attributes\n",
    "#             self._data.x = self._data.x[:, num_node_attributes:]\n",
    "#         if self._data.edge_attr is not None and not use_edge_attr:\n",
    "#             num_edge_attrs = self.num_edge_attributes\n",
    "#             self._data.edge_attr = self._data.edge_attr[:, num_edge_attrs:]\n",
    "\n",
    "#     @property\n",
    "#     def raw_dir(self) -> str:\n",
    "#         name = f'raw{\"_cleaned\" if self.cleaned else \"\"}'\n",
    "#         return osp.join(self.root, self.name, name)\n",
    "\n",
    "#     @property\n",
    "#     def processed_dir(self) -> str:\n",
    "#         name = f'processed{\"_cleaned\" if self.cleaned else \"\"}'\n",
    "#         return osp.join(self.root, self.name, name)\n",
    "\n",
    "#     @property\n",
    "#     def num_node_labels(self) -> int:\n",
    "#         return self.sizes['num_node_labels']\n",
    "\n",
    "#     @property\n",
    "#     def num_node_attributes(self) -> int:\n",
    "#         return self.sizes['num_node_attributes']\n",
    "\n",
    "#     @property\n",
    "#     def num_edge_labels(self) -> int:\n",
    "#         return self.sizes['num_edge_labels']\n",
    "\n",
    "#     @property\n",
    "#     def num_edge_attributes(self) -> int:\n",
    "#         return self.sizes['num_edge_attributes']\n",
    "\n",
    "#     @property\n",
    "#     def raw_file_names(self) -> List[str]:\n",
    "#         names = ['A', 'graph_indicator']\n",
    "#         return [f'{self.name}_{name}.txt' for name in names]\n",
    "\n",
    "#     @property\n",
    "#     def processed_file_names(self) -> str:\n",
    "#         return 'data.pt'\n",
    "        \n",
    "        \n",
    "#         #Original Function\n",
    "# #     def download(self) -> None:\n",
    "# #         url = self.cleaned_url if self.cleaned else self.url\n",
    "# #         fs.cp(f'{url}/{self.name}.zip', self.raw_dir, extract=True)\n",
    "# #         for filename in fs.ls(osp.join(self.raw_dir, self.name)):\n",
    "# #             fs.mv(filename, osp.join(self.raw_dir, osp.basename(filename)))\n",
    "# #         fs.rm(osp.join(self.raw_dir, self.name))\n",
    "        \n",
    "#     #Dummy Function for already downloaded files\n",
    "#     def download(self) -> None:\n",
    "#         files = {\n",
    "#     'ENZYMES_A.txt': 'https://raw.githubusercontent.com/snap-stanford/GraphRNN/master/dataset/ENZYMES/ENZYMES_A.txt',\n",
    "#     'ENZYMES_graph_indicator.txt': 'https://raw.githubusercontent.com/snap-stanford/GraphRNN/master/dataset/ENZYMES/ENZYMES_graph_indicator.txt',\n",
    "#     'ENZYMES_graph_labels.txt': 'https://raw.githubusercontent.com/snap-stanford/GraphRNN/master/dataset/ENZYMES/ENZYMES_graph_labels.txt',\n",
    "#     'ENZYMES_node_attributes.txt': 'https://raw.githubusercontent.com/snap-stanford/GraphRNN/master/dataset/ENZYMES/ENZYMES_node_attributes.txt',\n",
    "#     'ENZYMES_node_labels.txt': 'https://raw.githubusercontent.com/snap-stanford/GraphRNN/master/dataset/ENZYMES/ENZYMES_node_labels.txt'\n",
    "#     }\n",
    "#         dest_dir = '/kaggle/working/ENZYMES/raw/'\n",
    "\n",
    "#         # Ensure destination directory exists\n",
    "#         if not os.path.exists(dest_dir):\n",
    "#             os.makedirs(dest_dir)\n",
    "\n",
    "#         # Function to download and save a file\n",
    "#         def download_and_save(url, destination):\n",
    "#             # Make the HTTP GET request to the file URL\n",
    "#             if os.dir.exists()\n",
    "#             response = requests.get(url)\n",
    "#             if response.status_code == 200:\n",
    "#                 # Write the file contents in binary mode\n",
    "#                 filename = os.path.join(destination, url.split('/')[-1])\n",
    "#                 with open(filename, 'wb') as file:\n",
    "#                     file.write(response.content)\n",
    "#                 print(f\"File saved as {filename}\")\n",
    "#             else:\n",
    "#                 print(f\"Failed to download {url}\")\n",
    "\n",
    "#         # Download and save each file\n",
    "#         for file_url in files.values():\n",
    "#             download_and_save(file_url, dest_dir)\n",
    "\n",
    "#     def process(self) -> None:\n",
    "#         self.data, self.slices, sizes = read_tu_data(self.raw_dir, self.name)\n",
    "#         print(self.data.cos_sim)\n",
    "#         if self.pre_filter is not None or self.pre_transform is not None:\n",
    "#             data_list = [self.get(idx) for idx in range(len(self))]\n",
    "\n",
    "#             if self.pre_filter is not None:\n",
    "#                 data_list = [d for d in data_list if self.pre_filter(d)]\n",
    "\n",
    "#             if self.pre_transform is not None:\n",
    "#                 data_list = [self.pre_transform(d) for d in data_list]\n",
    "\n",
    "#             self.data, self.slices = self.collate(data_list)\n",
    "#             self._data_list = None  # Reset cache.\n",
    "\n",
    "#         assert isinstance(self._data, Data)\n",
    "#         fs.torch_save(\n",
    "#             (self._data.to_dict(), self.slices, sizes, self._data.__class__),\n",
    "#             self.processed_paths[0],\n",
    "#         )\n",
    "\n",
    "#     def __repr__(self) -> str:\n",
    "#         return f'{self.name}({len(self)})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e2fcb40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:03:27.184447Z",
     "iopub.status.busy": "2024-06-16T13:03:27.184062Z",
     "iopub.status.idle": "2024-06-16T13:03:29.657027Z",
     "shell.execute_reply": "2024-06-16T13:03:29.655623Z"
    },
    "papermill": {
     "duration": 2.486834,
     "end_time": "2024-06-16T13:03:29.659880",
     "exception": false,
     "start_time": "2024-06-16T13:03:27.173046",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://www.chrsmrrs.com/graphkerneldatasets/PROTEINS.zip\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "data = TUDataset(root=f'/working/{DATASET}', name=f'{DATASET}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d0f5b9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:03:29.683015Z",
     "iopub.status.busy": "2024-06-16T13:03:29.681312Z",
     "iopub.status.idle": "2024-06-16T13:03:29.688493Z",
     "shell.execute_reply": "2024-06-16T13:03:29.687091Z"
    },
    "papermill": {
     "duration": 0.021526,
     "end_time": "2024-06-16T13:03:29.691589",
     "exception": false,
     "start_time": "2024-06-16T13:03:29.670063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import aiohttp  # Import aiohttp to access ClientConnectorError\n",
    "# from requests.exceptions import SSLError\n",
    "\n",
    "# try:\n",
    "#     from torch_geometric.datasets import TUDataset\n",
    "#     data = TUDataset(root='/working/NCI1', name='NCI1')\n",
    "# except aiohttp.ClientConnectorSSLError as e:  # Catch connection errors specifically\n",
    "#     print(f\"Connection error occurred: {e}\")\n",
    "#     # Specify the folder where your dataset files are located\n",
    "#     folder = '/kaggle/working/'\n",
    "#     # Specify the prefix used in your dataset files\n",
    "#     prefix = 'NCI1'\n",
    "#     # Call the function assuming MyTUDataset is properly defined and imported\n",
    "#     data = MyTUDataset(folder, prefix)\n",
    "# except requests.exceptions.SSLError as e:\n",
    "#     print(f\"Connection error occurred: {e}\")\n",
    "#     # Specify the folder where your dataset files are located\n",
    "#     folder = '/kaggle/working/'\n",
    "#     # Specify the prefix used in your dataset files\n",
    "#     prefix = 'NCI1'\n",
    "#     # Call the function assuming MyTUDataset is properly defined and imported\n",
    "#     data = MyTUDataset(folder, prefix)\n",
    "# except Exception as e:  # Catch any other exceptions\n",
    "#     print(f\"An unexpected error occurred: {e}\")\n",
    "#     folder = '/kaggle/working/'\n",
    "#     # Specify the prefix used in your dataset files\n",
    "#     prefix = 'NCI1'\n",
    "#     # Call the function assuming MyTUDataset is properly defined and imported\n",
    "#     data = MyTUDataset(folder, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a40202ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:03:29.712676Z",
     "iopub.status.busy": "2024-06-16T13:03:29.712254Z",
     "iopub.status.idle": "2024-06-16T13:03:29.717922Z",
     "shell.execute_reply": "2024-06-16T13:03:29.716586Z"
    },
    "papermill": {
     "duration": 0.019373,
     "end_time": "2024-06-16T13:03:29.720600",
     "exception": false,
     "start_time": "2024-06-16T13:03:29.701227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "ID_dataset = IDAddingDataset(data, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3dda9d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:03:29.742732Z",
     "iopub.status.busy": "2024-06-16T13:03:29.742308Z",
     "iopub.status.idle": "2024-06-16T13:03:29.755220Z",
     "shell.execute_reply": "2024-06-16T13:03:29.754100Z"
    },
    "papermill": {
     "duration": 0.026341,
     "end_time": "2024-06-16T13:03:29.757675",
     "exception": false,
     "start_time": "2024-06-16T13:03:29.731334",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 34], x=[10, 3], y=[1], id=2, num_edges=34, adj_mat=[10, 10])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ID_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd8bc72f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:03:29.779057Z",
     "iopub.status.busy": "2024-06-16T13:03:29.778610Z",
     "iopub.status.idle": "2024-06-16T13:03:35.002934Z",
     "shell.execute_reply": "2024-06-16T13:03:35.001584Z"
    },
    "papermill": {
     "duration": 5.238835,
     "end_time": "2024-06-16T13:03:35.006080",
     "exception": false,
     "start_time": "2024-06-16T13:03:29.767245",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings_dict = {}\n",
    "for graph in ID_dataset:\n",
    "    embeddings_dict[graph.id] = get_WL_embedding(graph, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b908742e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:03:35.027553Z",
     "iopub.status.busy": "2024-06-16T13:03:35.027134Z",
     "iopub.status.idle": "2024-06-16T13:03:35.033045Z",
     "shell.execute_reply": "2024-06-16T13:03:35.031753Z"
    },
    "papermill": {
     "duration": 0.019311,
     "end_time": "2024-06-16T13:03:35.035212",
     "exception": false,
     "start_time": "2024-06-16T13:03:35.015901",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Embedding_dataset = EmbeddingAddingDataset(ID_dataset, embeddings_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "842d8bfc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:03:35.057531Z",
     "iopub.status.busy": "2024-06-16T13:03:35.057106Z",
     "iopub.status.idle": "2024-06-16T13:03:37.841124Z",
     "shell.execute_reply": "2024-06-16T13:03:37.839818Z"
    },
    "papermill": {
     "duration": 2.798022,
     "end_time": "2024-06-16T13:03:37.843870",
     "exception": false,
     "start_time": "2024-06-16T13:03:35.045848",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_hashes, all_classes, mean_vector = get_mean_vectors(Embedding_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5695d87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:03:37.866374Z",
     "iopub.status.busy": "2024-06-16T13:03:37.866004Z",
     "iopub.status.idle": "2024-06-16T13:03:37.873064Z",
     "shell.execute_reply": "2024-06-16T13:03:37.871697Z"
    },
    "papermill": {
     "duration": 0.020825,
     "end_time": "2024-06-16T13:03:37.875579",
     "exception": false,
     "start_time": "2024-06-16T13:03:37.854754",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 'all'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ba6a897",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:03:37.898922Z",
     "iopub.status.busy": "2024-06-16T13:03:37.898491Z",
     "iopub.status.idle": "2024-06-16T13:04:20.925757Z",
     "shell.execute_reply": "2024-06-16T13:04:20.924132Z"
    },
    "papermill": {
     "duration": 43.042173,
     "end_time": "2024-06-16T13:04:20.928954",
     "exception": false,
     "start_time": "2024-06-16T13:03:37.886781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cos_sims = {}\n",
    "class_cos_sims = {_class: [] for _class in all_classes}\n",
    "for graph in Embedding_dataset:\n",
    "    cos_sims[graph.id] = get_cos_sim(graph, all_hashes, all_classes, mean_vector)\n",
    "    class_cos_sims[graph.y.item()].append(cos_sims[graph.id]['class'])\n",
    "    class_cos_sims['all'].append(cos_sims[graph.id]['all'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed779e40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:04:20.950900Z",
     "iopub.status.busy": "2024-06-16T13:04:20.950484Z",
     "iopub.status.idle": "2024-06-16T13:04:20.958791Z",
     "shell.execute_reply": "2024-06-16T13:04:20.957317Z"
    },
    "papermill": {
     "duration": 0.022191,
     "end_time": "2024-06-16T13:04:20.961537",
     "exception": false,
     "start_time": "2024-06-16T13:04:20.939346",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.32312213722618655,\n",
       " 0.5294987022851299,\n",
       " 0.3611385099620866,\n",
       " 0.13077666414783173,\n",
       " 0.14397138256496927,\n",
       " 0.14539758423034188,\n",
       " 0.24593600725346534,\n",
       " 0.3664224054631915,\n",
       " 0.5754855051321773,\n",
       " 0.3970914344652682,\n",
       " 0.42590747829592984,\n",
       " 0.2173643648969267,\n",
       " 0.4278385948108817,\n",
       " 0.4018851933106837,\n",
       " 0.3892326390803181,\n",
       " 0.41682349878831587,\n",
       " 0.49949055044741564,\n",
       " 0.29072169182931296,\n",
       " 0.18168233781366364,\n",
       " 0.5287815736860608,\n",
       " 0.7440929654578191,\n",
       " 0.5128009953823301,\n",
       " 0.28534589933046933,\n",
       " 0.04535675574926221,\n",
       " 0.29346186175077893,\n",
       " 0.4358529503821466,\n",
       " 0.23813465334541653,\n",
       " 0.29824181683831563,\n",
       " 0.3303552579350117,\n",
       " 0.4564309386276454]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_cos_sims[0][:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5dbd990",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:04:20.983877Z",
     "iopub.status.busy": "2024-06-16T13:04:20.983453Z",
     "iopub.status.idle": "2024-06-16T13:04:22.341832Z",
     "shell.execute_reply": "2024-06-16T13:04:22.340680Z"
    },
    "papermill": {
     "duration": 1.372477,
     "end_time": "2024-06-16T13:04:22.344514",
     "exception": false,
     "start_time": "2024-06-16T13:04:20.972037",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_z_scores = {}\n",
    "\n",
    "for graph in Embedding_dataset:\n",
    "    all_z_scores[graph.id] = calc_z_scores(graph, cos_sims, class_cos_sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "707c2749",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:04:22.365982Z",
     "iopub.status.busy": "2024-06-16T13:04:22.365610Z",
     "iopub.status.idle": "2024-06-16T13:04:22.370957Z",
     "shell.execute_reply": "2024-06-16T13:04:22.369775Z"
    },
    "papermill": {
     "duration": 0.019013,
     "end_time": "2024-06-16T13:04:22.373589",
     "exception": false,
     "start_time": "2024-06-16T13:04:22.354576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the final transform and dataset\n",
    "Final_dataset = FinalAddingDataset(Embedding_dataset, cos_sims, all_z_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3df47d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:04:22.395730Z",
     "iopub.status.busy": "2024-06-16T13:04:22.395315Z",
     "iopub.status.idle": "2024-06-16T13:04:45.775162Z",
     "shell.execute_reply": "2024-06-16T13:04:45.773571Z"
    },
    "papermill": {
     "duration": 23.394282,
     "end_time": "2024-06-16T13:04:45.777921",
     "exception": false,
     "start_time": "2024-06-16T13:04:22.383639",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_node_dict = {_cls: {graph.id: graph.num_nodes for graph in Final_dataset if graph.y.item() == _cls or _cls == 'all'} for _cls in all_classes}\n",
    "num_node_mean_dict = {_cls: sum(num_node_dict[_cls].values())/len(num_node_dict[_cls].values()) for _cls in all_classes}\n",
    "node_diff_dict = {_cls: {graph.id: graph.num_nodes - num_node_mean_dict[_cls] for graph in Final_dataset if graph.y.item() == _cls or _cls == 'all'} for _cls in all_classes}\n",
    "pos_diff_dict = {_cls: {} for _cls in all_classes}\n",
    "neg_diff_dict = {_cls: {} for _cls in all_classes}\n",
    "for _cls, _dict in node_diff_dict.items():\n",
    "    for idx, node_diff in _dict.items():\n",
    "        if node_diff > 0:\n",
    "            pos_diff_dict[_cls][idx] = node_diff\n",
    "        else:\n",
    "            pos_diff_dict[_cls][idx] = node_diff\n",
    "pos_num_node_dict = {_cls: len(pos_diff_dict[_cls]) for _cls in all_classes}\n",
    "neg_num_node_dict = {_cls: len(neg_diff_dict[_cls]) for _cls in all_classes}\n",
    "percentile_dict = {graph.id: {'all': 0, 'class': 0} for graph in Final_dataset}\n",
    "for _cls, _dict in pos_diff_dict.items():\n",
    "    for i, (idx, val) in enumerate(sorted(_dict.items(), key = lambda x: x[1])):\n",
    "        if _cls != 'all':\n",
    "            percentile_dict[idx]['class'] = i/pos_num_node_dict[_cls]\n",
    "        else:\n",
    "            percentile_dict[idx]['all'] = i/pos_num_node_dict[_cls]\n",
    "for _cls, _dict in neg_diff_dict.items():\n",
    "    for i, (idx, val) in enumerate(sorted(_dict.items(), key = lambda x: x[1], reverse=True)):\n",
    "        if _cls != 'all':\n",
    "            percentile_dict[idx]['class'] = i/neg_num_node_dict[_cls]\n",
    "        else:\n",
    "            percentile_dict[idx]['all'] = i/neg_num_node_dict[_cls]\n",
    "    \n",
    "\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d40b00c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:04:45.800325Z",
     "iopub.status.busy": "2024-06-16T13:04:45.799918Z",
     "iopub.status.idle": "2024-06-16T13:04:45.806589Z",
     "shell.execute_reply": "2024-06-16T13:04:45.805396Z"
    },
    "papermill": {
     "duration": 0.020636,
     "end_time": "2024-06-16T13:04:45.808876",
     "exception": false,
     "start_time": "2024-06-16T13:04:45.788240",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'all': 0.7205750224618149, 'class': 0.6168929110105581}\n",
      "{'all': 0.5094339622641509, 'class': 0.358974358974359}\n",
      "{'all': 0.1096136567834681, 'class': 0.024132730015082957}\n",
      "{'all': 0.45103324348607365, 'class': 0.3031674208144796}\n",
      "{'all': 0.13926325247079965, 'class': 0.033182503770739065}\n",
      "{'all': 0.9964061096136568, 'class': 0.9939668174962293}\n",
      "{'all': 0.945193171608266, 'class': 0.9155354449472096}\n",
      "{'all': 0.9784366576819407, 'class': 0.9638009049773756}\n",
      "{'all': 0.31716082659478884, 'class': 0.1583710407239819}\n",
      "{'all': 0.14016172506738545, 'class': 0.03469079939668175}\n",
      "{'all': 0.33513027852650495, 'class': 0.1794871794871795}\n",
      "{'all': 0.7861635220125787, 'class': 0.6998491704374057}\n",
      "{'all': 0.3701707097933513, 'class': 0.21116138763197587}\n",
      "{'all': 0.738544474393531, 'class': 0.6425339366515838}\n",
      "{'all': 0.3360287511230907, 'class': 0.18099547511312217}\n",
      "{'all': 0.6999101527403414, 'class': 0.5942684766214178}\n",
      "{'all': 0.42138364779874216, 'class': 0.26244343891402716}\n",
      "{'all': 0.9946091644204852, 'class': 0.9909502262443439}\n",
      "{'all': 0.8095238095238095, 'class': 0.7285067873303167}\n",
      "{'all': 0.5103324348607368, 'class': 0.36048265460030166}\n"
     ]
    }
   ],
   "source": [
    "num_node_percentile_dict = percentile_dict\n",
    "for x in range(20):\n",
    "    print(num_node_percentile_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb6c8032",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:04:45.832709Z",
     "iopub.status.busy": "2024-06-16T13:04:45.831383Z",
     "iopub.status.idle": "2024-06-16T13:04:52.607106Z",
     "shell.execute_reply": "2024-06-16T13:04:52.605627Z"
    },
    "papermill": {
     "duration": 6.789904,
     "end_time": "2024-06-16T13:04:52.609811",
     "exception": false,
     "start_time": "2024-06-16T13:04:45.819907",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cos_sim_dict = {_cls: {} for _cls in all_classes}\n",
    "for graph in Final_dataset:\n",
    "    cos_sim_dict[graph.y.item()][graph.id] = graph.cos_sim['class']\n",
    "    cos_sim_dict['all'][graph.id] = graph.cos_sim['all']\n",
    "            \n",
    "\n",
    "cat_len_dict = {_cls: len(cos_sim_dict[_cls].values()) for _cls in all_classes}\n",
    "cos_sim_percentile_dict = {graph.id: {'all': 0, 'class': 0} for graph in Final_dataset}\n",
    "\n",
    "\n",
    "for _cls, _dict in cos_sim_dict.items():\n",
    "    for i, (idx, val) in enumerate(sorted(_dict.items(), key = lambda x: x[1])):\n",
    "        if _cls != 'all':\n",
    "            cos_sim_percentile_dict[idx]['class'] = i/cat_len_dict[_cls]\n",
    "        else:\n",
    "            cos_sim_percentile_dict[idx]['all'] = i/cat_len_dict[_cls]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5226caf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:04:52.631506Z",
     "iopub.status.busy": "2024-06-16T13:04:52.631111Z",
     "iopub.status.idle": "2024-06-16T13:04:52.636628Z",
     "shell.execute_reply": "2024-06-16T13:04:52.635564Z"
    },
    "papermill": {
     "duration": 0.019946,
     "end_time": "2024-06-16T13:04:52.639876",
     "exception": false,
     "start_time": "2024-06-16T13:04:52.619930",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'all': 0.5336927223719676, 'class': 0.4374057315233786}\n",
      "{'all': 0.9209344115004492, 'class': 0.8778280542986425}\n",
      "{'all': 0.6388140161725068, 'class': 0.526395173453997}\n",
      "{'all': 0.14555256064690028, 'class': 0.0889894419306184}\n",
      "{'all': 0.1967654986522911, 'class': 0.10105580693815988}\n",
      "{'all': 0.14106019766397124, 'class': 0.10407239819004525}\n",
      "{'all': 0.35399820305480684, 'class': 0.27601809954751133}\n",
      "{'all': 0.5992812219227314, 'class': 0.5369532428355958}\n",
      "{'all': 0.9622641509433962, 'class': 0.9457013574660633}\n",
      "{'all': 0.7241689128481581, 'class': 0.6093514328808446}\n"
     ]
    }
   ],
   "source": [
    "for x in range(10):\n",
    "    print(cos_sim_percentile_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "efbee069",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:04:52.662072Z",
     "iopub.status.busy": "2024-06-16T13:04:52.661706Z",
     "iopub.status.idle": "2024-06-16T13:04:52.706356Z",
     "shell.execute_reply": "2024-06-16T13:04:52.705041Z"
    },
    "papermill": {
     "duration": 0.058778,
     "end_time": "2024-06-16T13:04:52.708719",
     "exception": false,
     "start_time": "2024-06-16T13:04:52.649941",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "percentiles = [x for x in range(35, 91, 5)]\n",
    "cats = ['class', 'all']\n",
    "metrics = ['random', 'graph_order', 'cos_sim']\n",
    "train_indices_dict = {cat: {metric: {percentile: [] for percentile in percentiles} for metric in metrics} for cat in cats}\n",
    "for percentile in percentiles:\n",
    "    for idx, cat_pairs in cos_sim_percentile_dict.items():\n",
    "        for cat, val in cat_pairs.items():\n",
    "            if val < 0.01*percentile:\n",
    "                train_indices_dict[cat]['cos_sim'][percentile].append(idx)\n",
    "    for idx, cat_pairs in num_node_percentile_dict.items():\n",
    "        for cat, val in cat_pairs.items():\n",
    "            if val < 0.01* percentile:\n",
    "                train_indices_dict[cat]['graph_order'][percentile].append(idx)\n",
    "    for _cls, id_pairs in num_node_dict.items():\n",
    "        for cat in cats:\n",
    "            size = int(percentile * 0.01 * len(id_pairs))\n",
    "            train_indices_dict[cat]['random'][percentile] = np.random.choice(list(id_pairs.keys()), size, replace=False)\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7eeca959",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:04:52.731117Z",
     "iopub.status.busy": "2024-06-16T13:04:52.730685Z",
     "iopub.status.idle": "2024-06-16T13:04:52.744937Z",
     "shell.execute_reply": "2024-06-16T13:04:52.743887Z"
    },
    "papermill": {
     "duration": 0.028492,
     "end_time": "2024-06-16T13:04:52.747306",
     "exception": false,
     "start_time": "2024-06-16T13:04:52.718814",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Specify the path to the file where you want to save the dataset\n",
    "file_path = f'/kaggle/working/{DATASET}_train_indices_dict.pkl'\n",
    "\n",
    "# Saving the dataset\n",
    "with open(file_path, 'wb') as file:\n",
    "    pickle.dump(train_indices_dict, file)\n",
    "\n",
    "print(\"Dict saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "96d248fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:04:52.770089Z",
     "iopub.status.busy": "2024-06-16T13:04:52.769205Z",
     "iopub.status.idle": "2024-06-16T13:04:52.774827Z",
     "shell.execute_reply": "2024-06-16T13:04:52.773697Z"
    },
    "papermill": {
     "duration": 0.019288,
     "end_time": "2024-06-16T13:04:52.777020",
     "exception": false,
     "start_time": "2024-06-16T13:04:52.757732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# # for x in range(6):\n",
    "# all_z_scores = [graph.num_nodes for graph in Final_dataset]\n",
    "# num_node_mean = sum(all_z_scores)/600\n",
    "# all_z_scores = [x - num_node_mean for x in all_z_scores]\n",
    "# percentiles = [50, 95, 99]  # Change these values based on your requirements (xx%)\n",
    "# percentile_values = np.percentile(all_z_scores, percentiles)\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.hist(all_z_scores, bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "# plt.title('Histogram of Z-Scores with Percentiles')\n",
    "# plt.xlabel('Z-Score')\n",
    "# plt.ylabel('Frequency')\n",
    "\n",
    "# # Add vertical lines for each percentile\n",
    "# for perc, value in zip(percentiles, percentile_values):\n",
    "#     plt.axvline(x=value, color='r', linestyle='--', label=f'{perc}th percentile: {value:.2f}')\n",
    "\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fd08b945",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:04:52.799316Z",
     "iopub.status.busy": "2024-06-16T13:04:52.798733Z",
     "iopub.status.idle": "2024-06-16T13:04:56.457591Z",
     "shell.execute_reply": "2024-06-16T13:04:56.456032Z"
    },
    "papermill": {
     "duration": 3.672716,
     "end_time": "2024-06-16T13:04:56.460035",
     "exception": false,
     "start_time": "2024-06-16T13:04:52.787319",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Specify the path to the file where you want to save the dataset\n",
    "file_path = f'/kaggle/working/{DATASET}.pt'\n",
    "\n",
    "# Saving the dataset\n",
    "with open(file_path, 'wb') as file:\n",
    "    torch.save(Final_dataset, file)\n",
    "\n",
    "print(\"Dataset saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aa308d71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:04:56.483079Z",
     "iopub.status.busy": "2024-06-16T13:04:56.482216Z",
     "iopub.status.idle": "2024-06-16T13:04:56.488917Z",
     "shell.execute_reply": "2024-06-16T13:04:56.487715Z"
    },
    "papermill": {
     "duration": 0.020883,
     "end_time": "2024-06-16T13:04:56.491108",
     "exception": false,
     "start_time": "2024-06-16T13:04:56.470225",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of '/kaggle/working/':\n",
      "__notebook__.ipynb\n",
      "PROTEINS.pt\n",
      "PROTEINS_train_indices_dict.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the directory you want to list\n",
    "directory_path = '/kaggle/working/'\n",
    "\n",
    "# List all files and directories in the specified path\n",
    "contents = os.listdir(directory_path)\n",
    "\n",
    "print(\"Contents of '/kaggle/working/':\")\n",
    "for item in contents:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51380bfc",
   "metadata": {
    "papermill": {
     "duration": 0.010395,
     "end_time": "2024-06-16T13:04:56.511665",
     "exception": false,
     "start_time": "2024-06-16T13:04:56.501270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 119.04205,
   "end_time": "2024-06-16T13:04:57.847205",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-16T13:02:58.805155",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
