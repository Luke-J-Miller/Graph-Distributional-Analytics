{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0f21cc1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:03:18.634037Z",
     "iopub.status.busy": "2024-06-16T13:03:18.633637Z",
     "iopub.status.idle": "2024-06-16T13:03:33.518331Z",
     "shell.execute_reply": "2024-06-16T13:03:33.517176Z"
    },
    "papermill": {
     "duration": 14.897378,
     "end_time": "2024-06-16T13:03:33.520973",
     "exception": false,
     "start_time": "2024-06-16T13:03:18.623595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2+cpu)\r\n",
      "Collecting torch-geometric\r\n",
      "  Downloading torch_geometric-2.5.3-py3-none-any.whl.metadata (64 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12.1)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.3.1)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (4.66.4)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (1.26.4)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (1.11.4)\r\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (3.9.1)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (2.32.3)\r\n",
      "Requirement already satisfied: pyparsing in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (3.1.1)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (1.2.2)\r\n",
      "Requirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (5.9.3)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (23.2.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (6.0.4)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.9.3)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.4.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.3.1)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (4.0.3)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (2024.2.2)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->torch-geometric) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->torch-geometric) (3.2.0)\r\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\r\n",
      "Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: torch-geometric\r\n",
      "Successfully installed torch-geometric-2.5.3\r\n"
     ]
    }
   ],
   "source": [
    "DATASET = 'MUTAG'\n",
    "if 'first_run' not in globals():\n",
    "    !pip install torch torch-geometric\n",
    "    first_run = False\n",
    "else:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5257b066",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-06-16T13:03:33.540901Z",
     "iopub.status.busy": "2024-06-16T13:03:33.540508Z",
     "iopub.status.idle": "2024-06-16T13:03:40.406789Z",
     "shell.execute_reply": "2024-06-16T13:03:40.405806Z"
    },
    "papermill": {
     "duration": 6.879311,
     "end_time": "2024-06-16T13:03:40.409542",
     "exception": false,
     "start_time": "2024-06-16T13:03:33.530231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.transforms import BaseTransform\n",
    "from torch_geometric.utils import to_scipy_sparse_matrix\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.io import fs, read_txt_array\n",
    "import torch_geometric.transforms as T\n",
    "# from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "import math\n",
    "from typing import Callable, List, Optional, Tuple, Dict\n",
    "from torch import Tensor\n",
    "import os\n",
    "import requests\n",
    "from torch_geometric.utils import coalesce, cumsum, one_hot, remove_self_loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b77acb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:03:40.429600Z",
     "iopub.status.busy": "2024-06-16T13:03:40.429062Z",
     "iopub.status.idle": "2024-06-16T13:03:40.435051Z",
     "shell.execute_reply": "2024-06-16T13:03:40.433973Z"
    },
    "papermill": {
     "duration": 0.018622,
     "end_time": "2024-06-16T13:03:40.437193",
     "exception": false,
     "start_time": "2024-06-16T13:03:40.418571",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class MyTransform(BaseTransform):\n",
    "#     def __init__(self, set_ids = False, indices = None, set_initial_features=False, embedding=None, cos_sim=None, z_score=None):\n",
    "#         super(MyTransform, self).__init__()\n",
    "#         self.current_id = 0  # Initialize a counter for unique IDs\n",
    "#         self.set_initial_features = set_initial_features\n",
    "#         self.embedding = embedding  # Assume this is a function if not None\n",
    "#         self.cos_sim = cos_sim      # Assume this is a dictionary or list if not None\n",
    "#         self.z_score = z_score      # Assume this is a dictionary or list if not None\n",
    "#         self.indices = indices\n",
    "#         if set_ids:\n",
    "#             self.assign_ids()\n",
    "\n",
    "#     def assign_ids(self):\n",
    "#         # Create an ID tensor for all data points\n",
    "#         id_tensor = torch.arange(self.indices)\n",
    "#         for i, data in enumerate(self):\n",
    "#             data.id = id_tensor[i]\n",
    "\n",
    "#     def __call__(self, data: Data) -> Data:\n",
    "#         # Assign a unique ID and increment the counter\n",
    "#         if self.set_initial_features:\n",
    "#             data.id = self.current_id\n",
    "#             self.current_id += 1\n",
    "#             data.num_edges = data.edge_index.size(1)  # Number of edges\n",
    "#             data.adj_mat = self.to_numpy_adj(data)  # Convert edge index to adjacency matrix\n",
    "\n",
    "#         if self.embedding:\n",
    "#             data.embedding = self.embedding[data.id] # Assume embedding function takes Data and modifies it\n",
    "\n",
    "#         if self.cos_sim is not None:\n",
    "#             data.cos_sim = self.cos_sim[data.id]  # Fetch cosine similarity based on ID\n",
    "\n",
    "#         if self.z_score is not None:\n",
    "#             data.z_score = self.z_score[data.id]  # Fetch z-score based on ID\n",
    "\n",
    "#         return data\n",
    "\n",
    "#     def to_numpy_adj(self, data):\n",
    "#         # Creates an adjacency matrix from edge_index\n",
    "#         adj_mat = np.zeros((data.num_nodes, data.num_nodes))\n",
    "#         edge_index = data.edge_index.numpy()\n",
    "#         adj_mat[edge_index[0], edge_index[1]] = 1\n",
    "#         return adj_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb73ec97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:03:40.456699Z",
     "iopub.status.busy": "2024-06-16T13:03:40.456292Z",
     "iopub.status.idle": "2024-06-16T13:03:40.463955Z",
     "shell.execute_reply": "2024-06-16T13:03:40.462856Z"
    },
    "papermill": {
     "duration": 0.019915,
     "end_time": "2024-06-16T13:03:40.466101",
     "exception": false,
     "start_time": "2024-06-16T13:03:40.446186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class IDAddingDataset:\n",
    "    def __init__(self, dataset, attr_func):\n",
    "        self.dataset = dataset\n",
    "        self.attr_func = attr_func\n",
    "        self.indices = np.arange(0, len(dataset.indices()))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.dataset[idx].clone()  # Clone the data to avoid modifying the original dataset\n",
    "        data.id = self.indices[idx]\n",
    "        data.num_edges = self.dataset[idx].edge_index.size(1)\n",
    "        data.adj_mat = to_scipy_sparse_matrix(self.dataset[idx].edge_index, num_nodes=self.dataset[idx].num_nodes).toarray().astype(int)\n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d33f73c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:03:40.486143Z",
     "iopub.status.busy": "2024-06-16T13:03:40.485254Z",
     "iopub.status.idle": "2024-06-16T13:03:40.492035Z",
     "shell.execute_reply": "2024-06-16T13:03:40.490860Z"
    },
    "papermill": {
     "duration": 0.019035,
     "end_time": "2024-06-16T13:03:40.494158",
     "exception": false,
     "start_time": "2024-06-16T13:03:40.475123",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EmbeddingAddingDataset:\n",
    "    def __init__(self, dataset, embedding_dict):\n",
    "        self.dataset = dataset\n",
    "        self.embedding_dict = embedding_dict\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.dataset[idx].clone()  # Clone the data to avoid modifying the original dataset\n",
    "        data.embedding = self.embedding_dict[self.dataset[idx].id]\n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "806dd475",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:03:40.514343Z",
     "iopub.status.busy": "2024-06-16T13:03:40.513190Z",
     "iopub.status.idle": "2024-06-16T13:03:40.519975Z",
     "shell.execute_reply": "2024-06-16T13:03:40.519054Z"
    },
    "papermill": {
     "duration": 0.018974,
     "end_time": "2024-06-16T13:03:40.522108",
     "exception": false,
     "start_time": "2024-06-16T13:03:40.503134",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FinalAddingDataset:\n",
    "    def __init__(self, Embedding_dataset, cos_sims, all_z_scores):\n",
    "        self.dataset = Embedding_dataset\n",
    "        self.cos_sims = cos_sims\n",
    "        self.all_z_scores = all_z_scores\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.dataset[idx].clone()  # Clone the data to avoid modifying the original dataset\n",
    "        data.cos_sim = self.cos_sims[self.dataset[idx].id]\n",
    "        data.z_score = self.all_z_scores[self.dataset[idx].id]\n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6bbd7a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:03:40.541464Z",
     "iopub.status.busy": "2024-06-16T13:03:40.541080Z",
     "iopub.status.idle": "2024-06-16T13:03:40.549488Z",
     "shell.execute_reply": "2024-06-16T13:03:40.548408Z"
    },
    "papermill": {
     "duration": 0.020653,
     "end_time": "2024-06-16T13:03:40.551742",
     "exception": false,
     "start_time": "2024-06-16T13:03:40.531089",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def stable_hash(value):\n",
    "    return hashlib.md5(str(value).encode()).hexdigest()\n",
    "def get_WL_embedding(data, n_iter):\n",
    "    graph = data.adj_mat\n",
    "    graph_hash_dict = {}\n",
    "    labels = [np.sum(graph[x]) for x in range(len(graph))]  # Initialize labels based on node degrees\n",
    "    for _ in range(n_iter):\n",
    "        neighbor_labels = [sorted([labels[j] for j in range(len(graph)) if graph[i, j] == 1]) for i in range(len(graph))]\n",
    "        hashes = np.array([stable_hash((labels[i], tuple(neighbor_labels[i]))) for i in range(len(graph))])\n",
    "        \n",
    "        for unique_hash in set(hashes):\n",
    "            graph_hash_dict[unique_hash] = np.sum(hashes == unique_hash)\n",
    "\n",
    "        labels = hashes.tolist()\n",
    "\n",
    "    return graph_hash_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23a491c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:03:40.571834Z",
     "iopub.status.busy": "2024-06-16T13:03:40.570972Z",
     "iopub.status.idle": "2024-06-16T13:03:40.584979Z",
     "shell.execute_reply": "2024-06-16T13:03:40.583734Z"
    },
    "papermill": {
     "duration": 0.026363,
     "end_time": "2024-06-16T13:03:40.587141",
     "exception": false,
     "start_time": "2024-06-16T13:03:40.560778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_mean_vectors(dataset):\n",
    "    all_hashes = set()\n",
    "    all_classes = set()\n",
    "    for graph in dataset:\n",
    "        all_hashes.update(graph.embedding.keys())\n",
    "        all_classes.add(graph.y.item())\n",
    "    all_classes.add('all')\n",
    "\n",
    "    vector_sum_dict = {_cls: {h: 0 for h in all_hashes} for _cls in all_classes}\n",
    "    category_sum_dict = {_cls: 0 for _cls in all_classes}\n",
    "    mean_vector_dict = {_cls: {h: 0 for h in all_hashes} for _cls in all_classes}\n",
    "    \n",
    "    \n",
    "    for graph in dataset:\n",
    "        for _hash, count in graph.embedding.items():\n",
    "            vector_sum_dict[graph.y.item()][_hash] += count\n",
    "            category_sum_dict[graph.y.item()] += 1\n",
    "            vector_sum_dict['all'][_hash] += count\n",
    "            category_sum_dict['all'] += 1\n",
    "    for _cls, hash_counts in vector_sum_dict.items():\n",
    "        for _hash, count in hash_counts.items():\n",
    "            mean_vector_dict[_cls][_hash] = count/category_sum_dict[_cls]\n",
    "    return all_hashes, all_classes, mean_vector_dict\n",
    "\n",
    "def get_cos_sim(dataset, all_hashes, all_classes, mean_vectors):\n",
    "    cos_sims, norm_dict = {}, {}\n",
    "    graph_norm = math.sqrt(sum(count**2 for count in graph.embedding.values()))\n",
    "    for _class in all_classes:\n",
    "        norm_dict[_class] = math.sqrt(sum(count**2 for count in mean_vectors[_class].values()))\n",
    "    all_numerator = sum(count * mean_vectors['all'][_hash] for _hash, count in graph.embedding.items())\n",
    "    all_denom = norm_dict['all'] * graph_norm\n",
    "    cos_sims['all'] = all_numerator / all_denom\n",
    "\n",
    "    class_numerator = sum(count * mean_vectors[graph.y.item()][_hash] for _hash, count in graph.embedding.items())\n",
    "    class_denom = norm_dict[graph.y.item()] * graph_norm\n",
    "    cos_sims['class'] = class_numerator / class_denom\n",
    "    \n",
    "    return cos_sims\n",
    "\n",
    "def calc_z_scores(graph, cos_sims, class_cos_sims):\n",
    "    # Ensure data types are numpy arrays for statistical computation\n",
    "    all_cos_sims = np.array(class_cos_sims['all'])\n",
    "    cat_cos_sims = np.array(class_cos_sims[graph.y.item()])\n",
    "    \n",
    "    # Calculate means and standard deviations for 'all' and specific 'class'\n",
    "    all_mean = np.mean(all_cos_sims)\n",
    "    cat_mean = np.mean(cat_cos_sims)\n",
    "    all_std_dev = np.std(all_cos_sims)\n",
    "    class_std_dev = np.std(cat_cos_sims)\n",
    "    \n",
    "    # Compute z-scores using the standard formula, include flooring to nearest 0.5 if needed\n",
    "    z_scores = {\n",
    "        'all': (cos_sims[graph.id]['all']) / all_std_dev,\n",
    "        'class': (cos_sims[graph.id]['class']) / class_std_dev\n",
    "    }\n",
    "    \n",
    "    return z_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d80bac85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:03:40.606697Z",
     "iopub.status.busy": "2024-06-16T13:03:40.606326Z",
     "iopub.status.idle": "2024-06-16T13:03:40.610682Z",
     "shell.execute_reply": "2024-06-16T13:03:40.609668Z"
    },
    "papermill": {
     "duration": 0.016744,
     "end_time": "2024-06-16T13:03:40.612880",
     "exception": false,
     "start_time": "2024-06-16T13:03:40.596136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from torch_geometric.io.tu import read_tu_data\n",
    "# data, slices, sizes = read_tu_data('/kaggle/working/', 'ENZYMES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f01d3df0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:03:40.632893Z",
     "iopub.status.busy": "2024-06-16T13:03:40.632495Z",
     "iopub.status.idle": "2024-06-16T13:03:40.648943Z",
     "shell.execute_reply": "2024-06-16T13:03:40.648000Z"
    },
    "papermill": {
     "duration": 0.029393,
     "end_time": "2024-06-16T13:03:40.651260",
     "exception": false,
     "start_time": "2024-06-16T13:03:40.621867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os.path as osp\n",
    "# from typing import Callable, List, Optional\n",
    "\n",
    "# from torch_geometric.data import Data, InMemoryDataset\n",
    "# from torch_geometric.io import fs#, read_tu_data\n",
    "\n",
    "# def read_file(\n",
    "#     folder: str,\n",
    "#     prefix: str,\n",
    "#     name: str,\n",
    "#     dtype: Optional[torch.dtype] = None,\n",
    "# ) -> Tensor:\n",
    "#     path = osp.join(folder, f'{prefix}_{name}.txt')\n",
    "#     return read_txt_array(path, sep=',', dtype=dtype)\n",
    "# def cat(seq: List[Optional[Tensor]]) -> Optional[Tensor]:\n",
    "#     values = [v for v in seq if v is not None]\n",
    "#     values = [v for v in values if v.numel() > 0]\n",
    "#     values = [v.unsqueeze(-1) if v.dim() == 1 else v for v in values]\n",
    "#     return torch.cat(values, dim=-1) if len(values) > 0 else None\n",
    "# def split(data: Data, batch: Tensor) -> Tuple[Data, Dict[str, Tensor]]:\n",
    "#     node_slice = cumsum(torch.from_numpy(np.bincount(batch)))\n",
    "\n",
    "#     assert data.edge_index is not None\n",
    "#     row, _ = data.edge_index\n",
    "#     edge_slice = cumsum(torch.from_numpy(np.bincount(batch[row])))\n",
    "\n",
    "#     # Edge indices should start at zero for every graph.\n",
    "#     data.edge_index -= node_slice[batch[row]].unsqueeze(0)\n",
    "\n",
    "#     slices = {'edge_index': edge_slice}\n",
    "#     if data.x is not None:\n",
    "#         slices['x'] = node_slice\n",
    "#     else:\n",
    "#         # Imitate `collate` functionality:\n",
    "#         data._num_nodes = torch.bincount(batch).tolist()\n",
    "#         data.num_nodes = batch.numel()\n",
    "#     if data.edge_attr is not None:\n",
    "#         slices['edge_attr'] = edge_slice\n",
    "#     if data.y is not None:\n",
    "#         assert isinstance(data.y, Tensor)\n",
    "#         if data.y.size(0) == batch.size(0):\n",
    "#             slices['y'] = node_slice\n",
    "#         else:\n",
    "#             slices['y'] = torch.arange(0, int(batch[-1]) + 2, dtype=torch.long)\n",
    "\n",
    "#     return data, slices\n",
    "# def read_tu_data(\n",
    "#     folder: str,\n",
    "#     prefix: str,\n",
    "# ) -> Tuple[Data, Dict[str, Tensor], Dict[str, int]]:\n",
    "#     files = fs.glob(osp.join(folder, f'{prefix}_*.txt'))\n",
    "#     names = [osp.basename(f)[len(prefix) + 1:-4] for f in files]\n",
    "\n",
    "#     edge_index = read_file(folder, prefix, 'A', torch.long).t() - 1\n",
    "#     batch = read_file(folder, prefix, 'graph_indicator', torch.long) - 1\n",
    "\n",
    "#     node_attribute = torch.empty((batch.size(0), 0))\n",
    "#     if 'node_attributes' in names:\n",
    "#         node_attribute = read_file(folder, prefix, 'node_attributes')\n",
    "#         if node_attribute.dim() == 1:\n",
    "#             node_attribute = node_attribute.unsqueeze(-1)\n",
    "\n",
    "#     node_label = torch.empty((batch.size(0), 0))\n",
    "#     if 'node_labels' in names:\n",
    "#         node_label = read_file(folder, prefix, 'node_labels', torch.long)\n",
    "#         if node_label.dim() == 1:\n",
    "#             node_label = node_label.unsqueeze(-1)\n",
    "#         node_label = node_label - node_label.min(dim=0)[0]\n",
    "#         node_labels = list(node_label.unbind(dim=-1))\n",
    "#         node_labels = [one_hot(x) for x in node_labels]\n",
    "#         if len(node_labels) == 1:\n",
    "#             node_label = node_labels[0]\n",
    "#         else:\n",
    "#             node_label = torch.cat(node_labels, dim=-1)\n",
    "\n",
    "#     edge_attribute = torch.empty((edge_index.size(1), 0))\n",
    "#     if 'edge_attributes' in names:\n",
    "#         edge_attribute = read_file(folder, prefix, 'edge_attributes')\n",
    "#         if edge_attribute.dim() == 1:\n",
    "#             edge_attribute = edge_attribute.unsqueeze(-1)\n",
    "\n",
    "#     edge_label = torch.empty((edge_index.size(1), 0))\n",
    "#     if 'edge_labels' in names:\n",
    "#         edge_label = read_file(folder, prefix, 'edge_labels', torch.long)\n",
    "#         if edge_label.dim() == 1:\n",
    "#             edge_label = edge_label.unsqueeze(-1)\n",
    "#         edge_label = edge_label - edge_label.min(dim=0)[0]\n",
    "#         edge_labels = list(edge_label.unbind(dim=-1))\n",
    "#         edge_labels = [one_hot(e) for e in edge_labels]\n",
    "#         if len(edge_labels) == 1:\n",
    "#             edge_label = edge_labels[0]\n",
    "#         else:\n",
    "#             edge_label = torch.cat(edge_labels, dim=-1)\n",
    "\n",
    "#     x = cat([node_attribute, node_label])\n",
    "#     edge_attr = cat([edge_attribute, edge_label])\n",
    "\n",
    "#     y = None\n",
    "#     if 'graph_attributes' in names:  # Regression problem.\n",
    "#         y = read_file(folder, prefix, 'graph_attributes')\n",
    "#     elif 'graph_labels' in names:  # Classification problem.\n",
    "#         y = read_file(folder, prefix, 'graph_labels', torch.long)\n",
    "#         _, y = y.unique(sorted=True, return_inverse=True)\n",
    "\n",
    "#     num_nodes = int(edge_index.max()) + 1 if x is None else x.size(0)\n",
    "#     edge_index, edge_attr = remove_self_loops(edge_index, edge_attr)\n",
    "#     edge_index, edge_attr = coalesce(edge_index, edge_attr, num_nodes)\n",
    "#     cos_sim = torch.tensor([-1.0])\n",
    "#     z_score = torch.tensor([-1.0])\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#     data = Data(\n",
    "#         x=x,\n",
    "#         edge_index=edge_index,\n",
    "#         edge_attr=edge_attr,\n",
    "#         y=y,\n",
    "#         cos_sim=torch.tensor([-1.0]),  # Custom attribute\n",
    "#         z_score=torch.tensor([-1.0])   # Custom attribute\n",
    "#     )\n",
    "#     data, slices = split(data, batch)\n",
    "\n",
    "#     sizes = {\n",
    "#         'num_node_attributes': node_attribute.size(-1),\n",
    "#         'num_node_labels': node_label.size(-1),\n",
    "#         'num_edge_attributes': edge_attribute.size(-1),\n",
    "#         'num_edge_labels': edge_label.size(-1),\n",
    "#     }\n",
    "\n",
    "#     return data, slices, sizes\n",
    "\n",
    "# class MyTUDataset(InMemoryDataset):\n",
    "#     r\"\"\"A variety of graph kernel benchmark datasets, *.e.g.*,\n",
    "#     :obj:`\"IMDB-BINARY\"`, :obj:`\"REDDIT-BINARY\"` or :obj:`\"PROTEINS\"`,\n",
    "#     collected from the `TU Dortmund University\n",
    "#     <https://chrsmrrs.github.io/datasets>`_.\n",
    "#     In addition, this dataset wrapper provides `cleaned dataset versions\n",
    "#     <https://github.com/nd7141/graph_datasets>`_ as motivated by the\n",
    "#     `\"Understanding Isomorphism Bias in Graph Data Sets\"\n",
    "#     <https://arxiv.org/abs/1910.12091>`_ paper, containing only non-isomorphic\n",
    "#     graphs.\n",
    "\n",
    "#     .. note::\n",
    "#         Some datasets may not come with any node labels.\n",
    "#         You can then either make use of the argument :obj:`use_node_attr`\n",
    "#         to load additional continuous node attributes (if present) or provide\n",
    "#         synthetic node features using transforms such as\n",
    "#         :class:`torch_geometric.transforms.Constant` or\n",
    "#         :class:`torch_geometric.transforms.OneHotDegree`.\n",
    "\n",
    "#     Args:\n",
    "#         root (str): Root directory where the dataset should be saved.\n",
    "#         name (str): The `name\n",
    "#             <https://chrsmrrs.github.io/datasets/docs/datasets/>`_ of the\n",
    "#             dataset.\n",
    "#         transform (callable, optional): A function/transform that takes in an\n",
    "#             :obj:`torch_geometric.data.Data` object and returns a transformed\n",
    "#             version. The data object will be transformed before every access.\n",
    "#             (default: :obj:`None`)\n",
    "#         pre_transform (callable, optional): A function/transform that takes in\n",
    "#             an :obj:`torch_geometric.data.Data` object and returns a\n",
    "#             transformed version. The data object will be transformed before\n",
    "#             being saved to disk. (default: :obj:`None`)\n",
    "#         pre_filter (callable, optional): A function that takes in an\n",
    "#             :obj:`torch_geometric.data.Data` object and returns a boolean\n",
    "#             value, indicating whether the data object should be included in the\n",
    "#             final dataset. (default: :obj:`None`)\n",
    "#         force_reload (bool, optional): Whether to re-process the dataset.\n",
    "#             (default: :obj:`False`)\n",
    "#         use_node_attr (bool, optional): If :obj:`True`, the dataset will\n",
    "#             contain additional continuous node attributes (if present).\n",
    "#             (default: :obj:`False`)\n",
    "#         use_edge_attr (bool, optional): If :obj:`True`, the dataset will\n",
    "#             contain additional continuous edge attributes (if present).\n",
    "#             (default: :obj:`False`)\n",
    "#         cleaned (bool, optional): If :obj:`True`, the dataset will\n",
    "#             contain only non-isomorphic graphs. (default: :obj:`False`)\n",
    "\n",
    "#     **STATS:**\n",
    "\n",
    "#     .. list-table::\n",
    "#         :widths: 20 10 10 10 10 10\n",
    "#         :header-rows: 1\n",
    "\n",
    "#         * - Name\n",
    "#           - #graphs\n",
    "#           - #nodes\n",
    "#           - #edges\n",
    "#           - #features\n",
    "#           - #classes\n",
    "#         * - MUTAG\n",
    "#           - 188\n",
    "#           - ~17.9\n",
    "#           - ~39.6\n",
    "#           - 7\n",
    "#           - 2\n",
    "#         * - ENZYMES\n",
    "#           - 600\n",
    "#           - ~32.6\n",
    "#           - ~124.3\n",
    "#           - 3\n",
    "#           - 6\n",
    "#         * - PROTEINS\n",
    "#           - 1,113\n",
    "#           - ~39.1\n",
    "#           - ~145.6\n",
    "#           - 3\n",
    "#           - 2\n",
    "#         * - COLLAB\n",
    "#           - 5,000\n",
    "#           - ~74.5\n",
    "#           - ~4914.4\n",
    "#           - 0\n",
    "#           - 3\n",
    "#         * - IMDB-BINARY\n",
    "#           - 1,000\n",
    "#           - ~19.8\n",
    "#           - ~193.1\n",
    "#           - 0\n",
    "#           - 2\n",
    "#         * - REDDIT-BINARY\n",
    "#           - 2,000\n",
    "#           - ~429.6\n",
    "#           - ~995.5\n",
    "#           - 0\n",
    "#           - 2\n",
    "#         * - ...\n",
    "#           -\n",
    "#           -\n",
    "#           -\n",
    "#           -\n",
    "#           -\n",
    "#     \"\"\"\n",
    "\n",
    "#     url = 'https://www.chrsmrrs.com/graphkerneldatasets'\n",
    "#     cleaned_url = ('https://raw.githubusercontent.com/nd7141/'\n",
    "#                    'graph_datasets/master/datasets')\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         root: str,\n",
    "#         name: str,\n",
    "#         transform: Optional[Callable] = None,\n",
    "#         pre_transform: Optional[Callable] = None,\n",
    "#         pre_filter: Optional[Callable] = None,\n",
    "#         force_reload: bool = False,\n",
    "#         use_node_attr: bool = False,\n",
    "#         use_edge_attr: bool = False,\n",
    "#         cleaned: bool = False,\n",
    "#     ) -> None:\n",
    "#         self.name = name\n",
    "#         self.cleaned = cleaned\n",
    "#         super().__init__(root, transform, pre_transform, pre_filter,\n",
    "#                          force_reload=force_reload)\n",
    "\n",
    "#         out = fs.torch_load(self.processed_paths[0])\n",
    "#         if not isinstance(out, tuple) or len(out) < 3:\n",
    "#             raise RuntimeError(\n",
    "#                 \"The 'data' object was created by an older version of PyG. \"\n",
    "#                 \"If this error occurred while loading an already existing \"\n",
    "#                 \"dataset, remove the 'processed/' directory in the dataset's \"\n",
    "#                 \"root folder and try again.\")\n",
    "#         assert len(out) == 3 or len(out) == 4\n",
    "\n",
    "#         if len(out) == 3:  # Backward compatibility.\n",
    "#             data, self.slices, self.sizes = out\n",
    "#             data_cls = Data\n",
    "#         else:\n",
    "#             data, self.slices, self.sizes, data_cls = out\n",
    "\n",
    "#         if not isinstance(data, dict):  # Backward compatibility.\n",
    "#             self.data = data\n",
    "#         else:\n",
    "#             self.data = data_cls.from_dict(data)\n",
    "\n",
    "#         assert isinstance(self._data, Data)\n",
    "#         if self._data.x is not None and not use_node_attr:\n",
    "#             num_node_attributes = self.num_node_attributes\n",
    "#             self._data.x = self._data.x[:, num_node_attributes:]\n",
    "#         if self._data.edge_attr is not None and not use_edge_attr:\n",
    "#             num_edge_attrs = self.num_edge_attributes\n",
    "#             self._data.edge_attr = self._data.edge_attr[:, num_edge_attrs:]\n",
    "\n",
    "#     @property\n",
    "#     def raw_dir(self) -> str:\n",
    "#         name = f'raw{\"_cleaned\" if self.cleaned else \"\"}'\n",
    "#         return osp.join(self.root, self.name, name)\n",
    "\n",
    "#     @property\n",
    "#     def processed_dir(self) -> str:\n",
    "#         name = f'processed{\"_cleaned\" if self.cleaned else \"\"}'\n",
    "#         return osp.join(self.root, self.name, name)\n",
    "\n",
    "#     @property\n",
    "#     def num_node_labels(self) -> int:\n",
    "#         return self.sizes['num_node_labels']\n",
    "\n",
    "#     @property\n",
    "#     def num_node_attributes(self) -> int:\n",
    "#         return self.sizes['num_node_attributes']\n",
    "\n",
    "#     @property\n",
    "#     def num_edge_labels(self) -> int:\n",
    "#         return self.sizes['num_edge_labels']\n",
    "\n",
    "#     @property\n",
    "#     def num_edge_attributes(self) -> int:\n",
    "#         return self.sizes['num_edge_attributes']\n",
    "\n",
    "#     @property\n",
    "#     def raw_file_names(self) -> List[str]:\n",
    "#         names = ['A', 'graph_indicator']\n",
    "#         return [f'{self.name}_{name}.txt' for name in names]\n",
    "\n",
    "#     @property\n",
    "#     def processed_file_names(self) -> str:\n",
    "#         return 'data.pt'\n",
    "        \n",
    "        \n",
    "#         #Original Function\n",
    "# #     def download(self) -> None:\n",
    "# #         url = self.cleaned_url if self.cleaned else self.url\n",
    "# #         fs.cp(f'{url}/{self.name}.zip', self.raw_dir, extract=True)\n",
    "# #         for filename in fs.ls(osp.join(self.raw_dir, self.name)):\n",
    "# #             fs.mv(filename, osp.join(self.raw_dir, osp.basename(filename)))\n",
    "# #         fs.rm(osp.join(self.raw_dir, self.name))\n",
    "        \n",
    "#     #Dummy Function for already downloaded files\n",
    "#     def download(self) -> None:\n",
    "#         files = {\n",
    "#     'ENZYMES_A.txt': 'https://raw.githubusercontent.com/snap-stanford/GraphRNN/master/dataset/ENZYMES/ENZYMES_A.txt',\n",
    "#     'ENZYMES_graph_indicator.txt': 'https://raw.githubusercontent.com/snap-stanford/GraphRNN/master/dataset/ENZYMES/ENZYMES_graph_indicator.txt',\n",
    "#     'ENZYMES_graph_labels.txt': 'https://raw.githubusercontent.com/snap-stanford/GraphRNN/master/dataset/ENZYMES/ENZYMES_graph_labels.txt',\n",
    "#     'ENZYMES_node_attributes.txt': 'https://raw.githubusercontent.com/snap-stanford/GraphRNN/master/dataset/ENZYMES/ENZYMES_node_attributes.txt',\n",
    "#     'ENZYMES_node_labels.txt': 'https://raw.githubusercontent.com/snap-stanford/GraphRNN/master/dataset/ENZYMES/ENZYMES_node_labels.txt'\n",
    "#     }\n",
    "#         dest_dir = '/kaggle/working/ENZYMES/raw/'\n",
    "\n",
    "#         # Ensure destination directory exists\n",
    "#         if not os.path.exists(dest_dir):\n",
    "#             os.makedirs(dest_dir)\n",
    "\n",
    "#         # Function to download and save a file\n",
    "#         def download_and_save(url, destination):\n",
    "#             # Make the HTTP GET request to the file URL\n",
    "#             if os.dir.exists()\n",
    "#             response = requests.get(url)\n",
    "#             if response.status_code == 200:\n",
    "#                 # Write the file contents in binary mode\n",
    "#                 filename = os.path.join(destination, url.split('/')[-1])\n",
    "#                 with open(filename, 'wb') as file:\n",
    "#                     file.write(response.content)\n",
    "#                 print(f\"File saved as {filename}\")\n",
    "#             else:\n",
    "#                 print(f\"Failed to download {url}\")\n",
    "\n",
    "#         # Download and save each file\n",
    "#         for file_url in files.values():\n",
    "#             download_and_save(file_url, dest_dir)\n",
    "\n",
    "#     def process(self) -> None:\n",
    "#         self.data, self.slices, sizes = read_tu_data(self.raw_dir, self.name)\n",
    "#         print(self.data.cos_sim)\n",
    "#         if self.pre_filter is not None or self.pre_transform is not None:\n",
    "#             data_list = [self.get(idx) for idx in range(len(self))]\n",
    "\n",
    "#             if self.pre_filter is not None:\n",
    "#                 data_list = [d for d in data_list if self.pre_filter(d)]\n",
    "\n",
    "#             if self.pre_transform is not None:\n",
    "#                 data_list = [self.pre_transform(d) for d in data_list]\n",
    "\n",
    "#             self.data, self.slices = self.collate(data_list)\n",
    "#             self._data_list = None  # Reset cache.\n",
    "\n",
    "#         assert isinstance(self._data, Data)\n",
    "#         fs.torch_save(\n",
    "#             (self._data.to_dict(), self.slices, sizes, self._data.__class__),\n",
    "#             self.processed_paths[0],\n",
    "#         )\n",
    "\n",
    "#     def __repr__(self) -> str:\n",
    "#         return f'{self.name}({len(self)})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27f15f91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:03:40.671517Z",
     "iopub.status.busy": "2024-06-16T13:03:40.671125Z",
     "iopub.status.idle": "2024-06-16T13:03:41.171284Z",
     "shell.execute_reply": "2024-06-16T13:03:41.170122Z"
    },
    "papermill": {
     "duration": 0.512844,
     "end_time": "2024-06-16T13:03:41.173726",
     "exception": false,
     "start_time": "2024-06-16T13:03:40.660882",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://www.chrsmrrs.com/graphkerneldatasets/MUTAG.zip\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "data = TUDataset(root=f'/working/{DATASET}', name=f'{DATASET}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28ba6a6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:03:41.194179Z",
     "iopub.status.busy": "2024-06-16T13:03:41.193576Z",
     "iopub.status.idle": "2024-06-16T13:03:41.199486Z",
     "shell.execute_reply": "2024-06-16T13:03:41.198400Z"
    },
    "papermill": {
     "duration": 0.018591,
     "end_time": "2024-06-16T13:03:41.201699",
     "exception": false,
     "start_time": "2024-06-16T13:03:41.183108",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import aiohttp  # Import aiohttp to access ClientConnectorError\n",
    "# from requests.exceptions import SSLError\n",
    "\n",
    "# try:\n",
    "#     from torch_geometric.datasets import TUDataset\n",
    "#     data = TUDataset(root='/working/NCI1', name='NCI1')\n",
    "# except aiohttp.ClientConnectorSSLError as e:  # Catch connection errors specifically\n",
    "#     print(f\"Connection error occurred: {e}\")\n",
    "#     # Specify the folder where your dataset files are located\n",
    "#     folder = '/kaggle/working/'\n",
    "#     # Specify the prefix used in your dataset files\n",
    "#     prefix = 'NCI1'\n",
    "#     # Call the function assuming MyTUDataset is properly defined and imported\n",
    "#     data = MyTUDataset(folder, prefix)\n",
    "# except requests.exceptions.SSLError as e:\n",
    "#     print(f\"Connection error occurred: {e}\")\n",
    "#     # Specify the folder where your dataset files are located\n",
    "#     folder = '/kaggle/working/'\n",
    "#     # Specify the prefix used in your dataset files\n",
    "#     prefix = 'NCI1'\n",
    "#     # Call the function assuming MyTUDataset is properly defined and imported\n",
    "#     data = MyTUDataset(folder, prefix)\n",
    "# except Exception as e:  # Catch any other exceptions\n",
    "#     print(f\"An unexpected error occurred: {e}\")\n",
    "#     folder = '/kaggle/working/'\n",
    "#     # Specify the prefix used in your dataset files\n",
    "#     prefix = 'NCI1'\n",
    "#     # Call the function assuming MyTUDataset is properly defined and imported\n",
    "#     data = MyTUDataset(folder, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40be17f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:03:41.221630Z",
     "iopub.status.busy": "2024-06-16T13:03:41.221221Z",
     "iopub.status.idle": "2024-06-16T13:03:41.226387Z",
     "shell.execute_reply": "2024-06-16T13:03:41.225370Z"
    },
    "papermill": {
     "duration": 0.017838,
     "end_time": "2024-06-16T13:03:41.228627",
     "exception": false,
     "start_time": "2024-06-16T13:03:41.210789",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "ID_dataset = IDAddingDataset(data, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92dfb402",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:03:41.248239Z",
     "iopub.status.busy": "2024-06-16T13:03:41.247830Z",
     "iopub.status.idle": "2024-06-16T13:03:41.263669Z",
     "shell.execute_reply": "2024-06-16T13:03:41.262700Z"
    },
    "papermill": {
     "duration": 0.028468,
     "end_time": "2024-06-16T13:03:41.266222",
     "exception": false,
     "start_time": "2024-06-16T13:03:41.237754",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 28], x=[13, 7], edge_attr=[28, 4], y=[1], id=2, num_edges=28, adj_mat=[13, 13])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ID_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81bc82c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:03:41.287124Z",
     "iopub.status.busy": "2024-06-16T13:03:41.286732Z",
     "iopub.status.idle": "2024-06-16T13:03:41.565102Z",
     "shell.execute_reply": "2024-06-16T13:03:41.564118Z"
    },
    "papermill": {
     "duration": 0.291669,
     "end_time": "2024-06-16T13:03:41.567759",
     "exception": false,
     "start_time": "2024-06-16T13:03:41.276090",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings_dict = {}\n",
    "for graph in ID_dataset:\n",
    "    embeddings_dict[graph.id] = get_WL_embedding(graph, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4333f002",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:03:41.587859Z",
     "iopub.status.busy": "2024-06-16T13:03:41.587494Z",
     "iopub.status.idle": "2024-06-16T13:03:41.592197Z",
     "shell.execute_reply": "2024-06-16T13:03:41.591224Z"
    },
    "papermill": {
     "duration": 0.017387,
     "end_time": "2024-06-16T13:03:41.594499",
     "exception": false,
     "start_time": "2024-06-16T13:03:41.577112",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Embedding_dataset = EmbeddingAddingDataset(ID_dataset, embeddings_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5392670b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:03:41.614572Z",
     "iopub.status.busy": "2024-06-16T13:03:41.614213Z",
     "iopub.status.idle": "2024-06-16T13:03:41.970710Z",
     "shell.execute_reply": "2024-06-16T13:03:41.969759Z"
    },
    "papermill": {
     "duration": 0.36936,
     "end_time": "2024-06-16T13:03:41.973230",
     "exception": false,
     "start_time": "2024-06-16T13:03:41.603870",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_hashes, all_classes, mean_vector = get_mean_vectors(Embedding_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7be25562",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:03:41.993110Z",
     "iopub.status.busy": "2024-06-16T13:03:41.992714Z",
     "iopub.status.idle": "2024-06-16T13:03:41.999383Z",
     "shell.execute_reply": "2024-06-16T13:03:41.998470Z"
    },
    "papermill": {
     "duration": 0.019064,
     "end_time": "2024-06-16T13:03:42.001488",
     "exception": false,
     "start_time": "2024-06-16T13:03:41.982424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 'all'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e15fcda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:03:42.021599Z",
     "iopub.status.busy": "2024-06-16T13:03:42.021214Z",
     "iopub.status.idle": "2024-06-16T13:03:42.314669Z",
     "shell.execute_reply": "2024-06-16T13:03:42.313690Z"
    },
    "papermill": {
     "duration": 0.306653,
     "end_time": "2024-06-16T13:03:42.317425",
     "exception": false,
     "start_time": "2024-06-16T13:03:42.010772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cos_sims = {}\n",
    "class_cos_sims = {_class: [] for _class in all_classes}\n",
    "for graph in Embedding_dataset:\n",
    "    cos_sims[graph.id] = get_cos_sim(graph, all_hashes, all_classes, mean_vector)\n",
    "    class_cos_sims[graph.y.item()].append(cos_sims[graph.id]['class'])\n",
    "    class_cos_sims['all'].append(cos_sims[graph.id]['all'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "063728c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:03:42.337668Z",
     "iopub.status.busy": "2024-06-16T13:03:42.337254Z",
     "iopub.status.idle": "2024-06-16T13:03:42.344144Z",
     "shell.execute_reply": "2024-06-16T13:03:42.343144Z"
    },
    "papermill": {
     "duration": 0.019612,
     "end_time": "2024-06-16T13:03:42.346396",
     "exception": false,
     "start_time": "2024-06-16T13:03:42.326784",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7427582895032673,\n",
       " 0.7427582895032673,\n",
       " 0.7242129245911507,\n",
       " 0.7724016279419188,\n",
       " 0.7922193191696759,\n",
       " 0.8036949947563209,\n",
       " 0.5733603268162413,\n",
       " 0.7945616653709451,\n",
       " 0.7945616653709451,\n",
       " 0.7279746956723551,\n",
       " 0.7324934653728141,\n",
       " 0.6707957760191426,\n",
       " 0.7427582895032673,\n",
       " 0.714201454819843,\n",
       " 0.7775767525259659,\n",
       " 0.7475172230574338,\n",
       " 0.8263487905952647,\n",
       " 0.7760116631313123,\n",
       " 0.7603400521923102,\n",
       " 0.7543222812188674,\n",
       " 0.7356767343949191,\n",
       " 0.8013478120081661,\n",
       " 0.7984654135415279,\n",
       " 0.6826950508390139,\n",
       " 0.7484866144289212,\n",
       " 0.7758379717144482,\n",
       " 0.7427582895032673,\n",
       " 0.7357857242844517,\n",
       " 0.8065171947739874,\n",
       " 0.6826950508390139]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_cos_sims[0][:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bad3b506",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:03:42.368983Z",
     "iopub.status.busy": "2024-06-16T13:03:42.367922Z",
     "iopub.status.idle": "2024-06-16T13:03:42.574321Z",
     "shell.execute_reply": "2024-06-16T13:03:42.573295Z"
    },
    "papermill": {
     "duration": 0.220858,
     "end_time": "2024-06-16T13:03:42.577079",
     "exception": false,
     "start_time": "2024-06-16T13:03:42.356221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_z_scores = {}\n",
    "\n",
    "for graph in Embedding_dataset:\n",
    "    all_z_scores[graph.id] = calc_z_scores(graph, cos_sims, class_cos_sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da5483a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:03:42.597496Z",
     "iopub.status.busy": "2024-06-16T13:03:42.597102Z",
     "iopub.status.idle": "2024-06-16T13:03:42.601973Z",
     "shell.execute_reply": "2024-06-16T13:03:42.600944Z"
    },
    "papermill": {
     "duration": 0.017598,
     "end_time": "2024-06-16T13:03:42.604130",
     "exception": false,
     "start_time": "2024-06-16T13:03:42.586532",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the final transform and dataset\n",
    "Final_dataset = FinalAddingDataset(Embedding_dataset, cos_sims, all_z_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92bad0aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:03:42.624787Z",
     "iopub.status.busy": "2024-06-16T13:03:42.624385Z",
     "iopub.status.idle": "2024-06-16T13:03:46.323736Z",
     "shell.execute_reply": "2024-06-16T13:03:46.322594Z"
    },
    "papermill": {
     "duration": 3.713047,
     "end_time": "2024-06-16T13:03:46.326830",
     "exception": false,
     "start_time": "2024-06-16T13:03:42.613783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_node_dict = {_cls: {graph.id: graph.num_nodes for graph in Final_dataset if graph.y.item() == _cls or _cls == 'all'} for _cls in all_classes}\n",
    "num_node_mean_dict = {_cls: sum(num_node_dict[_cls].values())/len(num_node_dict[_cls].values()) for _cls in all_classes}\n",
    "node_diff_dict = {_cls: {graph.id: graph.num_nodes - num_node_mean_dict[_cls] for graph in Final_dataset if graph.y.item() == _cls or _cls == 'all'} for _cls in all_classes}\n",
    "pos_diff_dict = {_cls: {} for _cls in all_classes}\n",
    "neg_diff_dict = {_cls: {} for _cls in all_classes}\n",
    "for _cls, _dict in node_diff_dict.items():\n",
    "    for idx, node_diff in _dict.items():\n",
    "        if node_diff > 0:\n",
    "            pos_diff_dict[_cls][idx] = node_diff\n",
    "        else:\n",
    "            pos_diff_dict[_cls][idx] = node_diff\n",
    "pos_num_node_dict = {_cls: len(pos_diff_dict[_cls]) for _cls in all_classes}\n",
    "neg_num_node_dict = {_cls: len(neg_diff_dict[_cls]) for _cls in all_classes}\n",
    "percentile_dict = {graph.id: {'all': 0, 'class': 0} for graph in Final_dataset}\n",
    "for _cls, _dict in pos_diff_dict.items():\n",
    "    for i, (idx, val) in enumerate(sorted(_dict.items(), key = lambda x: x[1])):\n",
    "        if _cls != 'all':\n",
    "            percentile_dict[idx]['class'] = i/pos_num_node_dict[_cls]\n",
    "        else:\n",
    "            percentile_dict[idx]['all'] = i/pos_num_node_dict[_cls]\n",
    "for _cls, _dict in neg_diff_dict.items():\n",
    "    for i, (idx, val) in enumerate(sorted(_dict.items(), key = lambda x: x[1], reverse=True)):\n",
    "        if _cls != 'all':\n",
    "            percentile_dict[idx]['class'] = i/neg_num_node_dict[_cls]\n",
    "        else:\n",
    "            percentile_dict[idx]['all'] = i/neg_num_node_dict[_cls]\n",
    "    \n",
    "\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d87467f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:03:46.354197Z",
     "iopub.status.busy": "2024-06-16T13:03:46.353108Z",
     "iopub.status.idle": "2024-06-16T13:03:46.359410Z",
     "shell.execute_reply": "2024-06-16T13:03:46.358125Z"
    },
    "papermill": {
     "duration": 0.020193,
     "end_time": "2024-06-16T13:03:46.361599",
     "exception": false,
     "start_time": "2024-06-16T13:03:46.341406",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'all': 0.40425531914893614, 'class': 0.168}\n",
      "{'all': 0.15425531914893617, 'class': 0.36507936507936506}\n",
      "{'all': 0.1595744680851064, 'class': 0.38095238095238093}\n",
      "{'all': 0.5212765957446809, 'class': 0.336}\n",
      "{'all': 0.010638297872340425, 'class': 0.031746031746031744}\n",
      "{'all': 0.9840425531914894, 'class': 0.976}\n",
      "{'all': 0.3191489361702128, 'class': 0.7301587301587301}\n",
      "{'all': 0.6063829787234043, 'class': 0.456}\n",
      "{'all': 0.0851063829787234, 'class': 0.25396825396825395}\n",
      "{'all': 0.4095744680851064, 'class': 0.176}\n",
      "{'all': 0.4148936170212766, 'class': 0.184}\n",
      "{'all': 0.6117021276595744, 'class': 0.464}\n",
      "{'all': 0.723404255319149, 'class': 0.608}\n",
      "{'all': 0.16489361702127658, 'class': 0.3968253968253968}\n",
      "{'all': 0.526595744680851, 'class': 0.344}\n",
      "{'all': 0.7287234042553191, 'class': 0.616}\n",
      "{'all': 0.015957446808510637, 'class': 0.047619047619047616}\n",
      "{'all': 0.42021276595744683, 'class': 0.192}\n",
      "{'all': 0.1702127659574468, 'class': 0.4126984126984127}\n",
      "{'all': 0.5, 'class': 0.304}\n"
     ]
    }
   ],
   "source": [
    "num_node_percentile_dict = percentile_dict\n",
    "for x in range(20):\n",
    "    print(num_node_percentile_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f75cf77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:03:46.383880Z",
     "iopub.status.busy": "2024-06-16T13:03:46.383510Z",
     "iopub.status.idle": "2024-06-16T13:03:47.441410Z",
     "shell.execute_reply": "2024-06-16T13:03:47.440423Z"
    },
    "papermill": {
     "duration": 1.07282,
     "end_time": "2024-06-16T13:03:47.444200",
     "exception": false,
     "start_time": "2024-06-16T13:03:46.371380",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cos_sim_dict = {_cls: {} for _cls in all_classes}\n",
    "for graph in Final_dataset:\n",
    "    cos_sim_dict[graph.y.item()][graph.id] = graph.cos_sim['class']\n",
    "    cos_sim_dict['all'][graph.id] = graph.cos_sim['all']\n",
    "            \n",
    "\n",
    "cat_len_dict = {_cls: len(cos_sim_dict[_cls].values()) for _cls in all_classes}\n",
    "cos_sim_percentile_dict = {graph.id: {'all': 0, 'class': 0} for graph in Final_dataset}\n",
    "\n",
    "\n",
    "for _cls, _dict in cos_sim_dict.items():\n",
    "    for i, (idx, val) in enumerate(sorted(_dict.items(), key = lambda x: x[1])):\n",
    "        if _cls != 'all':\n",
    "            cos_sim_percentile_dict[idx]['class'] = i/cat_len_dict[_cls]\n",
    "        else:\n",
    "            cos_sim_percentile_dict[idx]['all'] = i/cat_len_dict[_cls]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ad521f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:03:47.465103Z",
     "iopub.status.busy": "2024-06-16T13:03:47.464709Z",
     "iopub.status.idle": "2024-06-16T13:03:47.470293Z",
     "shell.execute_reply": "2024-06-16T13:03:47.469083Z"
    },
    "papermill": {
     "duration": 0.018688,
     "end_time": "2024-06-16T13:03:47.472656",
     "exception": false,
     "start_time": "2024-06-16T13:03:47.453968",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'all': 0.9361702127659575, 'class': 0.84}\n",
      "{'all': 0.5106382978723404, 'class': 0.47619047619047616}\n",
      "{'all': 0.5159574468085106, 'class': 0.49206349206349204}\n",
      "{'all': 0.7180851063829787, 'class': 0.72}\n",
      "{'all': 0.0851063829787234, 'class': 0.30158730158730157}\n",
      "{'all': 0.19148936170212766, 'class': 0.144}\n",
      "{'all': 0.42021276595744683, 'class': 0.6190476190476191}\n",
      "{'all': 0.35638297872340424, 'class': 0.224}\n",
      "{'all': 0.6595744680851063, 'class': 0.7301587301587301}\n",
      "{'all': 0.43617021276595747, 'class': 0.448}\n"
     ]
    }
   ],
   "source": [
    "for x in range(10):\n",
    "    print(cos_sim_percentile_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "27ab1d3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:03:47.493696Z",
     "iopub.status.busy": "2024-06-16T13:03:47.493322Z",
     "iopub.status.idle": "2024-06-16T13:03:47.511671Z",
     "shell.execute_reply": "2024-06-16T13:03:47.510718Z"
    },
    "papermill": {
     "duration": 0.031626,
     "end_time": "2024-06-16T13:03:47.513894",
     "exception": false,
     "start_time": "2024-06-16T13:03:47.482268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "percentiles = [x for x in range(35, 91, 5)]\n",
    "cats = ['class', 'all']\n",
    "metrics = ['random', 'graph_order', 'cos_sim']\n",
    "train_indices_dict = {cat: {metric: {percentile: [] for percentile in percentiles} for metric in metrics} for cat in cats}\n",
    "for percentile in percentiles:\n",
    "    for idx, cat_pairs in cos_sim_percentile_dict.items():\n",
    "        for cat, val in cat_pairs.items():\n",
    "            if val < 0.01*percentile:\n",
    "                train_indices_dict[cat]['cos_sim'][percentile].append(idx)\n",
    "    for idx, cat_pairs in num_node_percentile_dict.items():\n",
    "        for cat, val in cat_pairs.items():\n",
    "            if val < 0.01* percentile:\n",
    "                train_indices_dict[cat]['graph_order'][percentile].append(idx)\n",
    "    for _cls, id_pairs in num_node_dict.items():\n",
    "        for cat in cats:\n",
    "            size = int(percentile * 0.01 * len(id_pairs))\n",
    "            train_indices_dict[cat]['random'][percentile] = np.random.choice(list(id_pairs.keys()), size, replace=False)\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1edb3658",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:03:47.535182Z",
     "iopub.status.busy": "2024-06-16T13:03:47.534797Z",
     "iopub.status.idle": "2024-06-16T13:03:47.542443Z",
     "shell.execute_reply": "2024-06-16T13:03:47.541427Z"
    },
    "papermill": {
     "duration": 0.020904,
     "end_time": "2024-06-16T13:03:47.544738",
     "exception": false,
     "start_time": "2024-06-16T13:03:47.523834",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Specify the path to the file where you want to save the dataset\n",
    "file_path = f'/kaggle/working/{DATASET}_train_indices_dict.pkl'\n",
    "\n",
    "# Saving the dataset\n",
    "with open(file_path, 'wb') as file:\n",
    "    pickle.dump(train_indices_dict, file)\n",
    "\n",
    "print(\"Dict saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb1f76ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:03:47.565555Z",
     "iopub.status.busy": "2024-06-16T13:03:47.565183Z",
     "iopub.status.idle": "2024-06-16T13:03:47.570349Z",
     "shell.execute_reply": "2024-06-16T13:03:47.569278Z"
    },
    "papermill": {
     "duration": 0.018059,
     "end_time": "2024-06-16T13:03:47.572548",
     "exception": false,
     "start_time": "2024-06-16T13:03:47.554489",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# # for x in range(6):\n",
    "# all_z_scores = [graph.num_nodes for graph in Final_dataset]\n",
    "# num_node_mean = sum(all_z_scores)/600\n",
    "# all_z_scores = [x - num_node_mean for x in all_z_scores]\n",
    "# percentiles = [50, 95, 99]  # Change these values based on your requirements (xx%)\n",
    "# percentile_values = np.percentile(all_z_scores, percentiles)\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.hist(all_z_scores, bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "# plt.title('Histogram of Z-Scores with Percentiles')\n",
    "# plt.xlabel('Z-Score')\n",
    "# plt.ylabel('Frequency')\n",
    "\n",
    "# # Add vertical lines for each percentile\n",
    "# for perc, value in zip(percentiles, percentile_values):\n",
    "#     plt.axvline(x=value, color='r', linestyle='--', label=f'{perc}th percentile: {value:.2f}')\n",
    "\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "32a226cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:03:47.593783Z",
     "iopub.status.busy": "2024-06-16T13:03:47.593360Z",
     "iopub.status.idle": "2024-06-16T13:03:47.845970Z",
     "shell.execute_reply": "2024-06-16T13:03:47.844942Z"
    },
    "papermill": {
     "duration": 0.26596,
     "end_time": "2024-06-16T13:03:47.848236",
     "exception": false,
     "start_time": "2024-06-16T13:03:47.582276",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Specify the path to the file where you want to save the dataset\n",
    "file_path = f'/kaggle/working/{DATASET}.pt'\n",
    "\n",
    "# Saving the dataset\n",
    "with open(file_path, 'wb') as file:\n",
    "    torch.save(Final_dataset, file)\n",
    "\n",
    "print(\"Dataset saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d1c8a712",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T13:03:47.869735Z",
     "iopub.status.busy": "2024-06-16T13:03:47.868799Z",
     "iopub.status.idle": "2024-06-16T13:03:47.875518Z",
     "shell.execute_reply": "2024-06-16T13:03:47.874243Z"
    },
    "papermill": {
     "duration": 0.019844,
     "end_time": "2024-06-16T13:03:47.877752",
     "exception": false,
     "start_time": "2024-06-16T13:03:47.857908",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of '/kaggle/working/':\n",
      "__notebook__.ipynb\n",
      "MUTAG.pt\n",
      "MUTAG_train_indices_dict.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the directory you want to list\n",
    "directory_path = '/kaggle/working/'\n",
    "\n",
    "# List all files and directories in the specified path\n",
    "contents = os.listdir(directory_path)\n",
    "\n",
    "print(\"Contents of '/kaggle/working/':\")\n",
    "for item in contents:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6c004e",
   "metadata": {
    "papermill": {
     "duration": 0.00951,
     "end_time": "2024-06-16T13:03:47.897149",
     "exception": false,
     "start_time": "2024-06-16T13:03:47.887639",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 33.254929,
   "end_time": "2024-06-16T13:03:49.130019",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-16T13:03:15.875090",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
