{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7802981,"sourceType":"datasetVersion","datasetId":4564083}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install dask_expr","metadata":{"execution":{"iopub.status.busy":"2024-05-29T18:43:12.504229Z","iopub.execute_input":"2024-05-29T18:43:12.505430Z","iopub.status.idle":"2024-05-29T18:43:32.103843Z","shell.execute_reply.started":"2024-05-29T18:43:12.505381Z","shell.execute_reply":"2024-05-29T18:43:32.102536Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting dask_expr\n  Downloading dask_expr-1.1.1-py3-none-any.whl.metadata (2.4 kB)\nCollecting dask==2024.5.1 (from dask_expr)\n  Downloading dask-2024.5.1-py3-none-any.whl.metadata (3.8 kB)\nRequirement already satisfied: pyarrow>=7.0.0 in /opt/conda/lib/python3.10/site-packages (from dask_expr) (15.0.0)\nRequirement already satisfied: pandas>=2 in /opt/conda/lib/python3.10/site-packages (from dask_expr) (2.2.0)\nRequirement already satisfied: click>=8.1 in /opt/conda/lib/python3.10/site-packages (from dask==2024.5.1->dask_expr) (8.1.7)\nRequirement already satisfied: cloudpickle>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from dask==2024.5.1->dask_expr) (2.2.1)\nRequirement already satisfied: fsspec>=2021.09.0 in /opt/conda/lib/python3.10/site-packages (from dask==2024.5.1->dask_expr) (2024.2.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from dask==2024.5.1->dask_expr) (21.3)\nRequirement already satisfied: partd>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from dask==2024.5.1->dask_expr) (1.4.1)\nRequirement already satisfied: pyyaml>=5.3.1 in /opt/conda/lib/python3.10/site-packages (from dask==2024.5.1->dask_expr) (6.0.1)\nRequirement already satisfied: toolz>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from dask==2024.5.1->dask_expr) (0.12.1)\nRequirement already satisfied: importlib-metadata>=4.13.0 in /opt/conda/lib/python3.10/site-packages (from dask==2024.5.1->dask_expr) (6.11.0)\nRequirement already satisfied: numpy<2,>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from pandas>=2->dask_expr) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=2->dask_expr) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=2->dask_expr) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=2->dask_expr) (2023.4)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata>=4.13.0->dask==2024.5.1->dask_expr) (3.17.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->dask==2024.5.1->dask_expr) (3.1.1)\nRequirement already satisfied: locket in /opt/conda/lib/python3.10/site-packages (from partd>=1.2.0->dask==2024.5.1->dask_expr) (1.0.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=2->dask_expr) (1.16.0)\nDownloading dask_expr-1.1.1-py3-none-any.whl (205 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.4/205.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading dask-2024.5.1-py3-none-any.whl (1.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: dask, dask_expr\n  Attempting uninstall: dask\n    Found existing installation: dask 2024.2.0\n    Uninstalling dask-2024.2.0:\n      Successfully uninstalled dask-2024.2.0\nSuccessfully installed dask-2024.5.1 dask_expr-1.1.1\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install bitstring","metadata":{"execution":{"iopub.status.busy":"2024-05-29T18:43:32.107167Z","iopub.execute_input":"2024-05-29T18:43:32.107606Z","iopub.status.idle":"2024-05-29T18:43:48.053862Z","shell.execute_reply.started":"2024-05-29T18:43:32.107569Z","shell.execute_reply":"2024-05-29T18:43:48.052477Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting bitstring\n  Downloading bitstring-4.2.3-py3-none-any.whl.metadata (5.0 kB)\nCollecting bitarray<3.0.0,>=2.9.0 (from bitstring)\n  Downloading bitarray-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\nDownloading bitstring-4.2.3-py3-none-any.whl (71 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.7/71.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bitarray-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.3/288.3 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: bitarray, bitstring\nSuccessfully installed bitarray-2.9.2 bitstring-4.2.3\n","output_type":"stream"}]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport dask\nimport shutil\nimport h5py\ndask.config.set({'dataframe.query-planning': True})\nimport dask.dataframe as dd# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport gc\nimport hashlib\nfrom concurrent.futures import ProcessPoolExecutor\nimport os\nfrom scipy.sparse import csr_matrix\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nfrom tqdm.auto import tqdm\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-29T18:43:48.056386Z","iopub.execute_input":"2024-05-29T18:43:48.056859Z","iopub.status.idle":"2024-05-29T18:43:50.224887Z","shell.execute_reply.started":"2024-05-29T18:43:48.056811Z","shell.execute_reply":"2024-05-29T18:43:50.223949Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"/kaggle/input/ogbg-ppi-medium/ppa_graphs\n/kaggle/input/ogbg-ppi-medium/ogbg_ppi_medium/ogbg_ppi_medium/RELEASE_v1.txt\n/kaggle/input/ogbg-ppi-medium/ogbg_ppi_medium/ogbg_ppi_medium/split/species/valid.csv\n/kaggle/input/ogbg-ppi-medium/ogbg_ppi_medium/ogbg_ppi_medium/split/species/train.csv\n/kaggle/input/ogbg-ppi-medium/ogbg_ppi_medium/ogbg_ppi_medium/split/species/test.csv\n/kaggle/input/ogbg-ppi-medium/ogbg_ppi_medium/ogbg_ppi_medium/mapping/README.md\n/kaggle/input/ogbg-ppi-medium/ogbg_ppi_medium/ogbg_ppi_medium/mapping/graphidx2speciesid.csv\n/kaggle/input/ogbg-ppi-medium/ogbg_ppi_medium/ogbg_ppi_medium/raw/num-edge-list.csv\n/kaggle/input/ogbg-ppi-medium/ogbg_ppi_medium/ogbg_ppi_medium/raw/num-node-list.csv\n/kaggle/input/ogbg-ppi-medium/ogbg_ppi_medium/ogbg_ppi_medium/raw/edge-feat.csv\n/kaggle/input/ogbg-ppi-medium/ogbg_ppi_medium/ogbg_ppi_medium/raw/edge.csv\n/kaggle/input/ogbg-ppi-medium/ogbg_ppi_medium/ogbg_ppi_medium/raw/graph-label.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"from bitstring import Bits, BitArray, BitStream, pack\nimport math\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-05-29T20:36:27.919694Z","iopub.execute_input":"2024-05-29T20:36:27.920123Z","iopub.status.idle":"2024-05-29T20:36:27.926796Z","shell.execute_reply.started":"2024-05-29T20:36:27.920090Z","shell.execute_reply":"2024-05-29T20:36:27.925042Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport math\nfrom bitstring import BitArray\n\ndef Weisfeiler_Leman_with_bit_encoding(adj_mat, num_iter):\n    vertex_degrees = np.array(adj_mat.sum(axis=1)).flatten()\n    max_degree = max(vertex_degrees)\n    bit_length = math.ceil(math.log(max_degree + 1, 2))\n    num_vertices = len(vertex_degrees)\n    labels = [format(degree, f'0{bit_length}b') for degree in vertex_degrees]\n\n    for _ in range(num_iter):\n        new_labels = []\n        for v in range(num_vertices):\n            neighbors = np.flatnonzero(adj_mat[v])\n            neighbor_labels = sorted((labels[n] for n in neighbors), reverse=True)\n            new_label = ''.join([labels[v]] + neighbor_labels)\n            new_labels.append(new_label)\n        labels = new_labels\n    \n    labels = sorted(labels, reverse=True)\n    \n    num_iter_encoded = format(num_iter, '08b')\n    bit_length_encoded = format(bit_length, '016b')\n    labels_encoded = ''.join(labels)\n    \n    bit_string = num_iter_encoded + bit_length_encoded + labels_encoded\n    embedding_string = BitArray(bin=bit_string)\n            \n    return embedding_string\n","metadata":{"execution":{"iopub.status.busy":"2024-05-29T20:36:29.757647Z","iopub.execute_input":"2024-05-29T20:36:29.758102Z","iopub.status.idle":"2024-05-29T20:36:29.770585Z","shell.execute_reply.started":"2024-05-29T20:36:29.758071Z","shell.execute_reply":"2024-05-29T20:36:29.769071Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"nlist = [[1, 2, 3], [3, 2, 1]]\nstr(nlist)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-29T20:59:45.017900Z","iopub.execute_input":"2024-05-29T20:59:45.018320Z","iopub.status.idle":"2024-05-29T20:59:45.026524Z","shell.execute_reply.started":"2024-05-29T20:59:45.018291Z","shell.execute_reply":"2024-05-29T20:59:45.025232Z"},"trusted":true},"execution_count":78,"outputs":[{"execution_count":78,"output_type":"execute_result","data":{"text/plain":"'[[1, 2, 3], [3, 2, 1]]'"},"metadata":{}}]},{"cell_type":"code","source":"def decode_embedding_string(embedding_string):\n    num_iter = int(embedding_string[:8].bin, 2)  # extract how many iterations the WL algorithm ran\n    bit_length = int(embedding_string[8:24].bin, 2)  # extract the bit length for this graph\n    label_bit_string = embedding_string[24:].bin\n    num_digits = len(label_bit_string) // bit_length\n    digit_list = [int(label_bit_string[x * bit_length: (x + 1) * bit_length], 2) for x in range(num_digits)]\n    current_list = [x for x in digit_list]\n    embedding = []\n    \n    for n in range(num_iter):\n        new_list = []\n        while current_list:\n            head = current_list.pop(0)\n            head_val = head\n            while not isinstance(head_val, int):\n                head_val = head_val[0]\n            values = current_list[:head_val]\n            del current_list[:head_val]\n            new_list.append((head, values))\n        current_list = new_list\n    embedding = current_list\n    for n in range(num_iter - 1):\n        new_list = [label[0] for label in current_list]\n        embedding.extend(new_list)\n        current_list = new_list\n        \n    graph_dict = {}\n    for item in embedding:\n        if str(item) in graph_dict.keys():\n            graph_dict[str(item)] += 1\n        else:\n            graph_dict[str(item)] = 1\n    return graph_dict\n        \n            ","metadata":{"execution":{"iopub.status.busy":"2024-05-29T21:00:11.138188Z","iopub.execute_input":"2024-05-29T21:00:11.138568Z","iopub.status.idle":"2024-05-29T21:00:11.151870Z","shell.execute_reply.started":"2024-05-29T21:00:11.138540Z","shell.execute_reply":"2024-05-29T21:00:11.150429Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"decode_embedding_string(bit_string)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T21:00:11.712235Z","iopub.execute_input":"2024-05-29T21:00:11.712690Z","iopub.status.idle":"2024-05-29T21:00:11.721477Z","shell.execute_reply.started":"2024-05-29T21:00:11.712659Z","shell.execute_reply":"2024-05-29T21:00:11.720224Z"},"trusted":true},"execution_count":80,"outputs":[{"execution_count":80,"output_type":"execute_result","data":{"text/plain":"{'(((3, [1, 1, 1]), [(1, [3]), (1, [3]), (1, [3])]), [((1, [3]), [(3, [1, 1, 1])]), ((1, [3]), [(3, [1, 1, 1])]), ((1, [3]), [(3, [1, 1, 1])])])': 1,\n '(((1, [3]), [(3, [1, 1, 1])]), [((3, [1, 1, 1]), [(1, [3]), (1, [3]), (1, [3])])])': 3,\n '((3, [1, 1, 1]), [(1, [3]), (1, [3]), (1, [3])])': 1,\n '((1, [3]), [(3, [1, 1, 1])])': 3,\n '(3, [1, 1, 1])': 1,\n '(1, [3])': 3}"},"metadata":{}}]},{"cell_type":"code","source":"from bitstring import BitArray\n\ndef decode_embedding_string(embedding_string):   \n    \n    num_iter = int(embedding_string[:8].bin, 2)  # extract how many iterations the WL algorithm ran\n    bit_length = int(embedding_string[8:24].bin, 2)  # extract the bit length for this graph\n    label_bit_string = embedding_string[24:].bin\n    \n    digit_list = []\n    num_digits = len(label_bit_string) // bit_length\n    \n    for x in range(num_digits):\n        digit_list.append(int(label_bit_string[x * bit_length: (x + 1) * bit_length], 2))\n    \n    graph_dict = {}\n    for n in range(num_iter):\n        graph_dict[n] = {}\n    \n    while digit_list:\n        key = [digit_list[0]]\n        key_string = str(key[0])\n        \n        for n in range(num_iter):\n            if len(digit_list) <= len(key):\n                print('Breaking out of loop, not enough digits left')\n                break\n            key, new_key_string = find_key(digit_list, key_string, key)\n            \n            if new_key_string not in graph_dict.keys():\n                graph_dict[n][new_key_string] = 1\n            else:\n                graph_dict[n][new_key_string] += 1\n            key_string = new_key_string\n        digit_list = digit_list[len(key):]\n        print(f'Updated digit_list: {digit_list}')\n    \n    return graph_dict\n\ndef find_key(digit_list, key_string, key):\n    key_length = len(key)\n    val_length = sum(key)\n    if key_length + val_length > len(digit_list):\n        raise ValueError('Function should stop after all digits are read.')\n    new_val = digit_list[key_length: key_length + val_length]\n    val_string = \"{\"\n    i = 0\n    while i < len(new_val):\n        \n    key = digit_list[: key_length + val_length]\n    new_key_string = '[' + key_string + ':' + ','.join(map(str, new_val)) + ']'\n    return key, new_key_string\n","metadata":{"execution":{"iopub.status.busy":"2024-05-29T19:55:44.565484Z","iopub.execute_input":"2024-05-29T19:55:44.566313Z","iopub.status.idle":"2024-05-29T19:55:44.582153Z","shell.execute_reply.started":"2024-05-29T19:55:44.566278Z","shell.execute_reply":"2024-05-29T19:55:44.580850Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"adj_mat = np.array([\n    [0,1,0,0],\n    [1,0,1,1],\n    [0,1,0,0],\n    [0,1,0,0]])\nbit_string = Weisfeiler_Leman_with_bit_encoding(adj_mat, 3)\nbit_string.bin","metadata":{"execution":{"iopub.status.busy":"2024-05-29T19:55:45.358936Z","iopub.execute_input":"2024-05-29T19:55:45.359385Z","iopub.status.idle":"2024-05-29T19:55:45.368159Z","shell.execute_reply.started":"2024-05-29T19:55:45.359345Z","shell.execute_reply":"2024-05-29T19:55:45.366914Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"'00000011000000000000001011010101011101110111011111010101011111010101011111010101011111010101110101010111011101110111110101011101010101110111011101111101010111010101011101110111'"},"metadata":{}}]},{"cell_type":"code","source":"graph_dict = decode_embedding_string(bit_string)\nprint(graph_dict)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T19:55:45.968318Z","iopub.execute_input":"2024-05-29T19:55:45.968775Z","iopub.status.idle":"2024-05-29T19:55:45.976235Z","shell.execute_reply.started":"2024-05-29T19:55:45.968742Z","shell.execute_reply":"2024-05-29T19:55:45.974696Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Updated digit_list: [1, 3, 3, 1, 1, 1, 3, 1, 1, 1, 1, 3, 1, 3, 1, 3, 1, 3, 3, 1, 1, 1, 3, 1, 1, 1, 1, 3, 1, 3, 1, 3, 1, 3, 3, 1, 1, 1, 3, 1, 1, 1, 1, 3, 1, 3, 1, 3]\nUpdated digit_list: [1, 3, 3, 1, 1, 1, 3, 1, 1, 1, 1, 3, 1, 3, 1, 3, 1, 3, 3, 1, 1, 1, 3, 1, 1, 1, 1, 3, 1, 3, 1, 3]\nUpdated digit_list: [1, 3, 3, 1, 1, 1, 3, 1, 1, 1, 1, 3, 1, 3, 1, 3]\nUpdated digit_list: []\n{0: {'[3:1,1,1]': 1, '[1:3]': 1}, 1: {'[[3:1,1,1]:1,3,1,3,1,3]': 1, '[[1:3]:3,1,1,1]': 1}, 2: {'[[[3:1,1,1]:1,3,1,3,1,3]:1,3,3,1,1,1,1,3,3,1,1,1,1,3,3,1,1,1]': 1, '[[[1:3]:3,1,1,1]:3,1,1,1,1,3,1,3,1,3]': 1}}\n","output_type":"stream"}]},{"cell_type":"code","source":"def chunkify_graphs(graphs, n_iter, parallel):\n    if parallel:\n        num_cores = os.cpu_count()\n        num_graphs = len(graphs)\n        steps = []\n        steps.append(0)\n        step = 0\n        for i in range(num_cores):\n            step += num_graphs//num_cores\n            steps.append(step)\n        steps[-1] = num_graphs\n        for i in range(num_cores):\n            yield (graphs[steps[i]:steps[i+1]], n_iter)\n    else:\n        yield (graphs, n_iter)\n\n\ndef process_vertex(graph, vertex, vertex_labels):\n    neighbors = graph[vertex].indices\n    neighbor_labels = [vertex_labels[n] for n in neighbors]\n    sorted_neighbor_labels = sorted_neighbor_labels\n    concatenated_labels = ''.join(sorted(neighbor_labels))\n    return stable_hash(concatenated_labels)\ndef process_graph(graph, n_iter):\n    vertex_degrees = np.array(graph.sum(axis=1)).flatten()\n    vertex_labels = [degree for degree in vertex_degrees]\n    num_vertices = len(vertex_labels)\n    graph_hash_values = np.empty(shape=(n_iter, num_vertices), dtype='<U32')\n    for i in range(n_iter):\n        new_vertex_labels  = []\n        for vertex in range(num_vertices):\n            new_vertex_labels.append(process_vertex(graph, vertex, vertex_labels))\n        vertex_labels = np.array([vertex_label for vertex_label in new_vertex_labels])\n        graph_hash_values[i, :] = vertex_labels\n    hashes, counts = np.unique(graph_hash_values, return_counts = True)\n    graph_embedding_dict = dict(zip(hashes, counts))\n    return graph_embedding_dict\ndef process_chunk_of_graphs(chunk_of_graphs_and_n_iter):\n    chunk_of_graphs, n_iter = chunk_of_graphs_and_n_iter\n    \"\"\"Process a chunk of graphs and return their hash counts\"\"\"\n    results = []\n    for graph in chunk_of_graphs:\n        results.append(process_graph(graph, n_iter))\n    return results\ndef graph_dataset_WL_embedding(graphs, n_iter, parallel = False):\n    num_cores = os.cpu_count()\n    for graph in graphs:\n        if not isinstance(graph, csr_matrix):\n            raise TypeError(\"All graphs need to be csr_matrix\")\n\n    \n    # Calculate chunk size\n    chunks_of_graphs_and_n_iter = list(chunkify_graphs(graphs, n_iter, parallel))\n    chunk_results = []\n    if parallel:\n        with ProcessPoolExecutor() as executor:\n            # Map each chunk to a process\n            chunk_results = list(executor.map(process_chunk_of_graphs, chunks_of_graphs_and_n_iter))\n    else:\n        for chunk in chunks_of_graphs_and_n_iter:\n            chunk_results.extend(process_chunk_of_graphs(chunk))\n  \n    graph_embeddings = [embedding for chunk in chunk_results for embedding in chunk]\n    return graph_embeddings       \n ","metadata":{"execution":{"iopub.status.busy":"2024-03-11T19:01:39.50641Z","iopub.execute_input":"2024-03-11T19:01:39.507033Z","iopub.status.idle":"2024-03-11T19:01:39.528073Z","shell.execute_reply.started":"2024-03-11T19:01:39.506995Z","shell.execute_reply":"2024-03-11T19:01:39.526755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#graphidx2species = pd.read_csv('/kaggle/input/ogbg-ppi-medium/ogbg_ppi_medium/ogbg_ppi_medium/mapping/graphidx2speciesid.csv')\n#valid = pd.read_csv('/kaggle/input/ogbg-ppi-medium/ogbg_ppi_medium/ogbg_ppi_medium/split/species/valid.csv', header = None).astype(np.uint32)\n#test = pd.read_csv('/kaggle/input/ogbg-ppi-medium/ogbg_ppi_medium/ogbg_ppi_medium/split/species/test.csv', header = None).astype(np.uint32)\n#train = pd.read_csv('/kaggle/input/ogbg-ppi-medium/ogbg_ppi_medium/ogbg_ppi_medium/split/species/valid.csv', header = None).astype(np.uint32)\ngraph_label= pd.read_csv('/kaggle/input/ogbg-ppi-medium/ogbg_ppi_medium/ogbg_ppi_medium/raw/graph-label.csv', header = None).astype(np.uint8)\n\nnum_node_list = pd.read_csv('/kaggle/input/ogbg-ppi-medium/ogbg_ppi_medium/ogbg_ppi_medium/raw/num-node-list.csv', header = None).astype(np.uint16)\nnum_edge_list = pd.read_csv('/kaggle/input/ogbg-ppi-medium/ogbg_ppi_medium/ogbg_ppi_medium/raw/num-edge-list.csv', header = None).astype(np.uint64)\nedge = pd.read_csv('/kaggle/input/ogbg-ppi-medium/ogbg_ppi_medium/ogbg_ppi_medium/raw/edge.csv', header=None).astype(np.uint16)\nedge_feat = dd.read_csv('/kaggle/input/ogbg-ppi-medium/ogbg_ppi_medium/ogbg_ppi_medium/raw/edge-feat.csv', header = None).astype(np.float32)","metadata":{"execution":{"iopub.status.busy":"2024-03-11T19:01:39.531456Z","iopub.execute_input":"2024-03-11T19:01:39.531853Z","iopub.status.idle":"2024-03-11T19:03:39.568219Z","shell.execute_reply.started":"2024-03-11T19:01:39.531818Z","shell.execute_reply":"2024-03-11T19:03:39.563971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-03-11T19:03:39.572038Z","iopub.execute_input":"2024-03-11T19:03:39.572926Z","iopub.status.idle":"2024-03-11T19:03:39.816758Z","shell.execute_reply.started":"2024-03-11T19:03:39.572892Z","shell.execute_reply":"2024-03-11T19:03:39.813631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_node_list = num_node_list.to_numpy().flatten()\nnum_edge_list = num_edge_list.to_numpy().flatten()\ngraph_label = graph_label.to_numpy().flatten()\n","metadata":{"execution":{"iopub.status.busy":"2024-03-11T19:03:39.82086Z","iopub.execute_input":"2024-03-11T19:03:39.821702Z","iopub.status.idle":"2024-03-11T19:03:39.841779Z","shell.execute_reply.started":"2024-03-11T19:03:39.821553Z","shell.execute_reply":"2024-03-11T19:03:39.837368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(graph_label)","metadata":{"execution":{"iopub.status.busy":"2024-03-11T19:03:39.848023Z","iopub.execute_input":"2024-03-11T19:03:39.851051Z","iopub.status.idle":"2024-03-11T19:03:39.872905Z","shell.execute_reply.started":"2024-03-11T19:03:39.850994Z","shell.execute_reply":"2024-03-11T19:03:39.869107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"edge_feat = edge_feat.compute()","metadata":{"execution":{"iopub.status.busy":"2024-03-11T19:03:39.876803Z","iopub.execute_input":"2024-03-11T19:03:39.877653Z","iopub.status.idle":"2024-03-11T19:06:54.541338Z","shell.execute_reply.started":"2024-03-11T19:03:39.877621Z","shell.execute_reply":"2024-03-11T19:06:54.539816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-03-11T19:06:54.542992Z","iopub.execute_input":"2024-03-11T19:06:54.54336Z","iopub.status.idle":"2024-03-11T19:06:54.66507Z","shell.execute_reply.started":"2024-03-11T19:06:54.543327Z","shell.execute_reply":"2024-03-11T19:06:54.664008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc_interval = 50\nwith h5py.File('graph_data.hdf5', 'w') as f:\n    num_edge_accum = 0  # Initialize accumulator for edge indices\n    for i, (num_node, num_edge) in enumerate(tqdm(zip(num_node_list, num_edge_list), total=len(num_node_list))):\n        # Create a group for each graph\n        grp = f.create_group(f'graph_{i}')\n        \n        # Generate the CSR matrix\n        start, stop = int(num_edge_accum), int(num_edge_accum + num_edge)\n        rows = edge.iloc[start:stop, 0].values\n        cols = edge.iloc[start:stop, 1].values\n        data = np.ones(len(rows), dtype=bool)\n        csr_graph = csr_matrix((data, (rows, cols)), shape=(num_node, num_node))\n        \n        embedding = process_graph(csr_graph, 3)\n        embedding_list_bytes = [(hash.encode('utf-8'), count) for hash, count in list(embedding.items())]\n        dt = np.dtype([('hash', 'S32'), ('count', np.int32)])  # 'S32' for 32-byte strings\n        embedding_array = np.array(embedding_list_bytes, dtype=dt)\n        grp.create_dataset('embedding', data = embedding_array)\n        \n        # Store CSR matrix components in the group\n        grp.create_dataset('data', data=csr_graph.data)\n        grp.create_dataset('indices', data=csr_graph.indices)\n        grp.create_dataset('indptr', data=csr_graph.indptr)\n        grp.create_dataset('shape', data=csr_graph.shape)\n        \n        # Store edge features\n        edge_feats = edge_feat.iloc[start:stop, :].values\n        grp.create_dataset('edge_feat', data=edge_feats)\n        \n        # Optionally, store other graph attributes\n        grp.attrs['num_nodes'] = num_node\n        grp.attrs['num_edges'] = num_edge\n        grp.attrs['graph_label'] = graph_label[i]\n        \n        # Update accumulators\n        num_edge_accum += num_edge\n\n        # Optional: Clear memory, if necessary\n        if (i + 1) % gc_interval == 0:\n            gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-03-11T19:10:45.089173Z","iopub.execute_input":"2024-03-11T19:10:45.089597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\nimport os\n\n# Define source paths\nsrc_graphidx2species = '/kaggle/input/ogbg-ppi-medium/ogbg_ppi_medium/ogbg_ppi_medium/mapping/graphidx2speciesid.csv'\nsrc_valid = '/kaggle/input/ogbg-ppi-medium/ogbg_ppi_medium/ogbg_ppi_medium/split/species/valid.csv'\nsrc_test = '/kaggle/input/ogbg-ppi-medium/ogbg_ppi_medium/ogbg_ppi_medium/split/species/test.csv'\nsrc_train = '/kaggle/input/ogbg-ppi-medium/ogbg_ppi_medium/ogbg_ppi_medium/split/species/train.csv'  # Assuming this path is correct and meant to be 'valid.csv'\n\n# Define destination directory\ndest_dir = '/kaggle/working/'\n\n# Ensure the destination directory exists\nos.makedirs(dest_dir, exist_ok=True)\n\n# Copy files\nshutil.copy(src_graphidx2species, os.path.join(dest_dir, 'graphidx2speciesid.csv'))\nshutil.copy(src_valid, os.path.join(dest_dir, 'valid.csv'))\nshutil.copy(src_test, os.path.join(dest_dir, 'test.csv'))\nshutil.copy(src_train, os.path.join(dest_dir, 'train.csv'))  # Note: This copies 'valid.csv' as 'train.csv' based on your paths. Adjust if needed.\n","metadata":{},"execution_count":null,"outputs":[]}]}